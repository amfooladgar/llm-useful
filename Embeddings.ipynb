{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc477b3f",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe6d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 204.4869\n",
      "Epoch 100, Loss: 119.8568\n",
      "Epoch 200, Loss: 112.9452\n",
      "Epoch 300, Loss: 112.0204\n",
      "Epoch 400, Loss: 111.7854\n",
      "Epoch 500, Loss: 111.6963\n",
      "Epoch 600, Loss: 111.6511\n",
      "Epoch 700, Loss: 111.6222\n",
      "Epoch 800, Loss: 111.6004\n",
      "Epoch 900, Loss: 111.5823\n",
      "\n",
      "Vector for 'king':\n",
      "[ 0.94666437  0.73740089  0.78218087 -0.13031644 -0.51329574  1.50188143\n",
      "  0.39904126  1.57611522 -0.05634789  0.66174988]\n",
      "\n",
      "Vector for 'queen':\n",
      "[ 1.18255719  0.56217685  0.95310334  0.22998969 -0.38948249  1.09408709\n",
      "  0.153157    1.99713324 -0.10473538  1.34576386]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# ---------------------\n",
    "# 1. Prepare the corpus\n",
    "# ---------------------\n",
    "corpus = [\n",
    "    \"king is a strong man\",\n",
    "    \"queen is a wise woman\",\n",
    "    \"man and woman are humans\",\n",
    "    \"prince is a young king\",\n",
    "    \"princess is a young queen\"\n",
    "]\n",
    "\n",
    "# Tokenize each sentence into a list of lowercase words\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Build word frequency dictionary\n",
    "word_counts = defaultdict(int)\n",
    "for sentence in tokenized_corpus:\n",
    "    for word in sentence:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "# Create vocabulary and lookup tables\n",
    "vocab = list(word_counts.keys())\n",
    "word2idx = {w: idx for idx, w in enumerate(vocab)}    # word to index mapping\n",
    "idx2word = {idx: w for w, idx in word2idx.items()}    # index to word mapping\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Generate skip-gram pairs\n",
    "# ----------------------------\n",
    "def generate_training_data(tokenized_corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    For each word in a sentence, return (center, context) pairs within the given window.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                context_idx = idx + j\n",
    "                if j != 0 and 0 <= context_idx < len(sentence):\n",
    "                    context_word = sentence[context_idx]\n",
    "                    pairs.append((center_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "# Create training data\n",
    "training_data = generate_training_data(tokenized_corpus)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Initialize model weights\n",
    "# ----------------------------\n",
    "embedding_dim = 10   # Size of word embedding vector\n",
    "\n",
    "# Weight matrix from input to hidden layer\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Weight matrix from hidden to output layer\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Softmax and training\n",
    "# -----------------------\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Stable softmax function to convert raw scores into probabilities.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))  # stability trick\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def train(epochs=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train the model using naive softmax cross-entropy loss and SGD.\n",
    "    \"\"\"\n",
    "    global W1, W2\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for center, context in training_data:\n",
    "            # Create one-hot vector for center word\n",
    "            x = np.zeros(vocab_size)\n",
    "            x[word2idx[center]] = 1\n",
    "\n",
    "            # Forward pass\n",
    "            h = np.dot(W1.T, x)          # hidden layer (center word projection)\n",
    "            u = np.dot(W2.T, h)          # output layer scores\n",
    "            y_pred = softmax(u)          # predicted probability distribution\n",
    "\n",
    "            # True distribution (one-hot for context word)\n",
    "            y_true = np.zeros(vocab_size)\n",
    "            y_true[word2idx[context]] = 1\n",
    "\n",
    "            # Compute error\n",
    "            e = y_pred - y_true\n",
    "\n",
    "            # Backpropagate errors to weights\n",
    "            dW2 = np.outer(h, e)\n",
    "            dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "            # Update weights\n",
    "            W1 -= learning_rate * dW1\n",
    "            W2 -= learning_rate * dW2\n",
    "\n",
    "            # Accumulate loss\n",
    "            loss += -np.log(y_pred[word2idx[context]])\n",
    "        \n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train()\n",
    "\n",
    "# -------------------\n",
    "# 5. Query word vector\n",
    "# -------------------\n",
    "def get_word_vector(word):\n",
    "    \"\"\"\n",
    "    Return the learned embedding vector for a given word.\n",
    "    \"\"\"\n",
    "    idx = word2idx[word]\n",
    "    return W1[idx]\n",
    "\n",
    "# Show learned vectors for \"king\" and \"queen\"\n",
    "print(\"\\nVector for 'king':\")\n",
    "print(get_word_vector(\"king\"))\n",
    "\n",
    "print(\"\\nVector for 'queen':\")\n",
    "print(get_word_vector(\"queen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b07b77",
   "metadata": {},
   "source": [
    "# Embeddings with transformer models\n",
    "- BERT (Bidirectional Encoder Representations from Transformers): embeddings for word not sentence. BERT by itself wasn’t designed to produce a single vector for a whole sentence comparison out-of-the-box\n",
    "- S-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73238307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence_transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.9/site-packages (from sentence_transformers) (4.14.1)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2025.7.34-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.6.1-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "Downloading hf_xet-1.1.7-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading regex-2025.7.34-cp39-cp39-macosx_11_0_arm64.whl (285 kB)\n",
      "Downloading safetensors-0.6.1-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: mpmath, urllib3, tqdm, sympy, safetensors, regex, pyyaml, Pillow, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.3.0 certifi-2025.8.3 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.7 huggingface-hub-0.34.3 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.1 sentence_transformers-5.1.0 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 tqdm-4.67.1 transformers-4.55.0 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a68624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alifouladgar/Documents/llm-useful/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/alifouladgar/Documents/llm-useful/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "# Sentence BERT (S-BERT)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embedding = model.encode(sentence)\n",
    "print(embedding.shape)  # (384,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36547fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
