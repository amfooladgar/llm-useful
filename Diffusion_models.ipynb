{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f48231",
   "metadata": {},
   "source": [
    "# Diffusion Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce466c81",
   "metadata": {},
   "source": [
    "\n",
    "These three techniques are the main reasons diffusion models have become practical enough for production use.\n",
    "\n",
    "## üîπ 1. DDIM (Denoising Diffusion Implicit Models)\n",
    "\n",
    "* **Problem it solves:** Standard diffusion requires hundreds or thousands of denoising steps ‚Üí very slow generation.\n",
    "* **Key idea:** Instead of sampling with the full stochastic process, DDIM reformulates the reverse diffusion as a **deterministic mapping**, skipping steps while preserving quality.\n",
    "* **How it works:**\n",
    "\n",
    "  * In regular diffusion, each step adds some randomness.\n",
    "  * DDIM uses a non-Markovian formulation that allows you to **‚Äújump‚Äù across steps deterministically**.\n",
    "  * This means you can sample with **20‚Äì50 steps instead of 1000**.\n",
    "* **Impact:** Much faster sampling, often with comparable image quality. Used in most real-world deployments of Stable Diffusion.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Distillation\n",
    "\n",
    "* **Problem it solves:** Even with DDIM, inference is still slower than GANs (multiple steps needed).\n",
    "* **Key idea:** Train a **smaller, faster ‚Äústudent‚Äù model** to mimic the outputs of the large, multi-step diffusion model.\n",
    "* **Types of distillation in diffusion:**\n",
    "\n",
    "  * **Progressive distillation:** Train a model that can do in *N/2* steps what the original did in *N*. Repeat until very few steps remain.\n",
    "  * **One-step distillation:** Train a model to approximate the final output of a full diffusion run in a single forward pass.\n",
    "* **Impact:** Sampling can become **as fast as GANs** while keeping diffusion‚Äôs stability and fidelity. Still an active research area.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Classifier-Free Guidance (CFG)\n",
    "\n",
    "* **Problem it solves:** Diffusion models conditioned on prompts sometimes produce **weakly aligned outputs** (the image doesn‚Äôt match the text well).\n",
    "* **Key idea:** Train the model with and without conditioning (e.g., with text prompt and with null prompt). At inference:\n",
    "\n",
    "  * Generate two predictions (with prompt and without).\n",
    "  * Combine them with a guidance weight:\n",
    "\n",
    "    $$\n",
    "    \\hat{\\epsilon} = \\hat{\\epsilon}_\\text{uncond} + s \\cdot (\\hat{\\epsilon}_\\text{cond} - \\hat{\\epsilon}_\\text{uncond})\n",
    "    $$\n",
    "\n",
    "    where $s$ is the **guidance scale** (usually 5‚Äì10).\n",
    "* **Impact:** Strongly improves alignment between prompt and output (e.g., ‚Äúa cat in a spacesuit‚Äù actually looks like one).\n",
    "* **Trade-off:** Higher scale = better alignment, but less diversity (images start looking too similar).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why These Matter\n",
    "\n",
    "* **DDIM ‚Üí Faster inference** (practical for interactive apps).\n",
    "* **Distillation ‚Üí Near real-time generation** (closing gap with GANs).\n",
    "* **Classifier-Free Guidance ‚Üí Better controllability** (makes outputs useful for actual creative tasks).\n",
    "\n",
    "Together, they turn diffusion models from a **research novelty** into **deployable systems** that power tools like Stable Diffusion, Midjourney, and Photoshop‚Äôs AI features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab11cee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
