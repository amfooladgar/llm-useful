{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bd9e29",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c97f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(log_probs):\n",
    "    \"\"\"\n",
    "    Given a list of log probabilities (base e) for each token in a sequence,\n",
    "    compute the perplexity of the model for this sequence.\n",
    "    \"\"\"\n",
    "    N = len(log_probs)  # number of tokens\n",
    "    # Compute the average negative log-likelihood:\n",
    "    avg_neg_log_likelihood = -sum(log_probs)/N\n",
    "    # Compute perplexity:\n",
    "    perplexity = math.exp(avg_neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346e257a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9477340410546757"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "# If log_probs = [-1.2, -0.5, -0.3], then\n",
    "# sum(log_probs) = -2.0, N = 3,\n",
    "# avg_neg_log_likelihood = -(-2.0)/3 = 0.666...,\n",
    "# perplexity = exp(0.666...) ≈ 1.95.\n",
    "log_probs = [-1.2, -0.5, -0.3]\n",
    "calculate_perplexity(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b765bd",
   "metadata": {},
   "source": [
    "# BLEU: Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06b31b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.5789\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_bleu(candidate, references, max_n=4):\n",
    "\n",
    "    # Tokenize candidate and references\n",
    "    candidate_tokens = candidate.split()\n",
    "    references_tokens = [ref.split() for ref in references]\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        # Get n-grams for candidate\n",
    "        candidate_ngrams = Counter(tuple(candidate_tokens[i:i+n]) for i in range(len(candidate_tokens)-n+1))\n",
    "        max_ref_ngrams = Counter()\n",
    "\n",
    "        # Get max reference n-grams counts\n",
    "        for ref in references_tokens:\n",
    "            ref_ngrams = Counter(tuple(ref[i:i+n]) for i in range(len(ref)-n+1))\n",
    "            for ngram in ref_ngrams:\n",
    "                max_ref_ngrams[ngram] = max(max_ref_ngrams.get(ngram,0), ref_ngrams[ngram])\n",
    "\n",
    "        # Clip candidate n-gram counts by reference max counts\n",
    "        clipped_counts = {ngram: min(count, max_ref_ngrams.get(ngram,0)) for ngram, count in candidate_ngrams.items()}\n",
    "\n",
    "        precision = sum(clipped_counts.values()) / max(1, sum(candidate_ngrams.values()))\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # Geometric mean of precisions\n",
    "    if min(precisions) > 0:\n",
    "        geo_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "    else:\n",
    "        geo_mean = 0\n",
    "\n",
    "    # Brevity penalty\n",
    "    ref_lens = [len(ref) for ref in references_tokens]\n",
    "    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - len(candidate_tokens)), ref_len))\n",
    "    bp = math.exp(1 - closest_ref_len / len(candidate_tokens)) if len(candidate_tokens) < closest_ref_len else 1\n",
    "\n",
    "    return bp * geo_mean\n",
    "\n",
    "# Example\n",
    "candidate = \"the cat is on mat\"\n",
    "references = [\"the cat is on the mat\", \"there is a cat on the mat\"]\n",
    "\n",
    "print(f\"BLEU score: {compute_bleu(candidate, references):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d81f28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "candidate = \"cat is on mat\"\n",
    "references = [\"the cat sits on the mat\", \"there is a cat on the mat\"]\n",
    "\n",
    "print(f\"BLEU score: {compute_bleu(candidate, references):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab93532",
   "metadata": {},
   "source": [
    "Because you’re computing sentence-level BLEU without smoothing, the score collapses to 0 whenever any higher-order n-gram precision is 0. That happens a lot:\n",
    "\n",
    "Short candidates: if len(candidate_tokens) < n, there are no n-grams for that n ⇒ precision for that order = 0 ⇒ geometric mean = 0.\n",
    "\n",
    "Paraphrases / wording changes: candidate has no 3-gram/4-gram overlaps with the references ⇒ some p_n = 0 ⇒ overall BLEU = 0.\n",
    "\n",
    "Case / tokenization mismatches: “The” vs “the”, punctuation differences, etc., reduce overlaps and can zero out higher-order precisions.\n",
    "\n",
    "Single reference: fewer chances to match n-grams, increasing the odds of zeros.\n",
    "\n",
    "(Less likely) Empty candidate: your BP step will actually raise a division-by-zero error (not just return 0), because you divide by len(candidate_tokens).\n",
    "\n",
    "In your code, this line enforces the collapse:\n",
    "```\n",
    "if min(precisions) > 0:\n",
    "    geo_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "else:\n",
    "    geo_mean = 0\n",
    "\n",
    "```\n",
    "\n",
    "If any precision is 0 (very common for 3/4-grams), geo_mean is forced to 0.\n",
    "\n",
    "**Quick fixe:**\n",
    "1- Add smoothing (essential at sentence level)\n",
    "Replace the geometric-mean block with a smoothed version (Chen & Cherry–style epsilon smoothing is the simplest):\n",
    "```\n",
    "# Geometric mean of precisions with simple smoothing\n",
    "eps = 1e-9\n",
    "precisions_smoothed = [p if p > 0 else eps for p in precisions]\n",
    "geo_mean = math.exp(sum(math.log(p) for p in precisions_smoothed) / max_n)\n",
    "```\n",
    "2- Lower max_n or use weights\n",
    "\n",
    "or short sentences, consider BLEU-2 (bigrams) or weight lower orders more heavily.\n",
    "\n",
    "3- Normalize tokens\n",
    "\n",
    "Lowercase and strip punctuation consistently before n-gram counting.\n",
    "\n",
    "4- Use multiple references\n",
    "\n",
    "Increases chance of higher-order overlaps.\n",
    "\n",
    "5- Prefer corpus BLEU for reporting\n",
    "\n",
    "Aggregate n-gram counts over many sentences; zeros become much rarer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2da74178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_bleu(candidate, references, max_n=4, smooth=True, eps=1e-9):\n",
    "    # Tokenize\n",
    "    candidate_tokens = candidate.lower().split()\n",
    "    references_tokens = [ref.lower().split() for ref in references]\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        cand_ngrams = Counter(tuple(candidate_tokens[i:i+n]) for i in range(len(candidate_tokens)-n+1))\n",
    "        max_ref_ngrams = Counter()\n",
    "        for ref in references_tokens:\n",
    "            ref_ngrams = Counter(tuple(ref[i:i+n]) for i in range(len(ref)-n+1))\n",
    "            for ng, c in ref_ngrams.items():\n",
    "                if c > max_ref_ngrams[ng]:\n",
    "                    max_ref_ngrams[ng] = c\n",
    "\n",
    "        clipped = {ng: min(c, max_ref_ngrams.get(ng, 0)) for ng, c in cand_ngrams.items()}\n",
    "        match = sum(clipped.values())\n",
    "        total = sum(cand_ngrams.values())\n",
    "        precisions.append(match / total if total > 0 else 0.0)\n",
    "\n",
    "    # Geometric mean (with optional smoothing)\n",
    "    if smooth:\n",
    "        precisions = [p if p > 0 else eps for p in precisions]\n",
    "    if any(p <= 0 for p in precisions):\n",
    "        # no smoothing case hits here and becomes 0\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        geo_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "\n",
    "    # Brevity penalty (guard empty candidate)\n",
    "    c = len(candidate_tokens)\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    ref_lens = [len(ref) for ref in references_tokens]\n",
    "    closest_ref_len = min(ref_lens, key=lambda r: (abs(r - c), r))\n",
    "    bp = math.exp(1 - closest_ref_len / c) if c < closest_ref_len else 1.0\n",
    "\n",
    "    return bp * geo_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57e88977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "candidate = \"the cat is on mat\"\n",
    "references = [\"the cat sits on the mat\", \"there is a cat on the mat\"]\n",
    "\n",
    "print(f\"BLEU score: {compute_bleu(candidate, references):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da1c61",
   "metadata": {},
   "source": [
    "# Production-grade implementation of BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ae5bd",
   "metadata": {},
   "source": [
    "Above is not “best” for real evaluations. It’s missing a few things practitioners rely on:\n",
    "\n",
    "- Smoothing for zero precisions (critical at sentence level)\n",
    "- Configurable n-gram weights (e.g., BLEU-1, BLEU-2, BLEU-4)\n",
    "- Robust tokenization / case handling\n",
    "- Corpus-level BLEU (micro-averaging counts across a set)\n",
    "- Clear choice of effective reference length for BP (closest/shortest/average)\n",
    "- Safe handling when the candidate is shorter than n (no n-grams)\n",
    "\n",
    "Below is a more production-leaning drop-in that adds these, while staying minimal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8045c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Example          BLEU  \\\n",
      "0             Sentence BLEU-1 (no smoothing)  8.333333e-01   \n",
      "1       Short sentence BLEU-4 (no smoothing)  0.000000e+00   \n",
      "2   Short sentence BLEU-4 (with smoothing=1)  6.541924e-08   \n",
      "3   Sentence BLEU-4 with multiple references  5.789301e-01   \n",
      "4                       BP strategy: closest  2.408487e-03   \n",
      "5                      BP strategy: shortest  2.408487e-03   \n",
      "6                       BP strategy: average  2.408487e-03   \n",
      "7                      Custom weights BLEU-2  7.090416e-01   \n",
      "8                      Custom weights BLEU-4  5.789301e-01   \n",
      "9                Corpus BLEU-4 (3 sentences)  6.136894e-01   \n",
      "10                Case-insensitive tokenizer  1.000000e+00   \n",
      "11                  Case-sensitive tokenizer  1.000000e-09   \n",
      "\n",
      "                 candidate                                         references  \n",
      "0   The cat sat on the mat                            [The cat is on the mat]  \n",
      "1                    hello                                      [hello world]  \n",
      "2                    hello                                      [hello world]  \n",
      "3        the cat is on mat  [the cat is on the mat, there is a cat on the ...  \n",
      "4        the cat is on mat                 [the cat is definitely on the mat]  \n",
      "5        the cat is on mat                 [the cat is definitely on the mat]  \n",
      "6        the cat is on mat                 [the cat is definitely on the mat]  \n",
      "7        the cat is on mat  [the cat is on the mat, there is a cat on the ...  \n",
      "8        the cat is on mat  [the cat is on the mat, there is a cat on the ...  \n",
      "9                      NaN                                                NaN  \n",
      "10   The Cat Is On The Mat                            [the cat is on the mat]  \n",
      "11   The Cat Is On The Mat                            [the cat is on the mat]  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "from typing import Iterable, List, Sequence, Tuple, Callable, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Utility Functions ---------------- #\n",
    "def simple_tokenize(s: str) -> List[str]:\n",
    "    return s.lower().split()\n",
    "\n",
    "def ngrams(tokens: Sequence[str], n: int) -> Iterable[Tuple[str, ...]]:\n",
    "    L = len(tokens)\n",
    "    for i in range(max(0, L - n + 1)):\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def modified_precision(candidate_tokens: List[str],\n",
    "                       references_tokens: List[List[str]],\n",
    "                       n: int) -> Tuple[int, int]:\n",
    "    cand_counts = Counter(ngrams(candidate_tokens, n))\n",
    "    if not cand_counts:\n",
    "        return 0, 0\n",
    "\n",
    "    max_ref_counts = Counter()\n",
    "    for ref in references_tokens:\n",
    "        ref_counts = Counter(ngrams(ref, n))\n",
    "        for g, c in ref_counts.items():\n",
    "            max_ref_counts[g] = max(max_ref_counts.get(g, 0), c)\n",
    "\n",
    "    clipped = {g: min(c, max_ref_counts.get(g, 0)) for g, c in cand_counts.items()}\n",
    "    return sum(clipped.values()), sum(cand_counts.values())\n",
    "\n",
    "def brevity_penalty(candidate_len: int, ref_lens: List[int], strategy: str = \"closest\") -> float:\n",
    "    if candidate_len == 0:\n",
    "        return 0.0\n",
    "    if strategy == \"shortest\":\n",
    "        r = min(ref_lens)\n",
    "    elif strategy == \"average\":\n",
    "        r = sum(ref_lens) / len(ref_lens)\n",
    "    else:  # closest\n",
    "        r = min(ref_lens, key=lambda rl: (abs(rl - candidate_len), rl))\n",
    "    if candidate_len > r:\n",
    "        return 1.0\n",
    "    return math.exp(1 - (r / candidate_len))\n",
    "\n",
    "def chen_cherry_smoothing(p_list: List[float], method: int = 1) -> List[float]:\n",
    "    eps = 1e-9\n",
    "    if method == 1:\n",
    "        return [p if p > 0 else eps for p in p_list]\n",
    "    return p_list\n",
    "\n",
    "# ---------------- BLEU Functions ---------------- #\n",
    "def compute_sentence_bleu(candidate: str,\n",
    "                          references: Sequence[str],\n",
    "                          max_n: int = 4,\n",
    "                          weights: Optional[Sequence[float]] = None,\n",
    "                          tokenizer: Callable[[str], List[str]] = simple_tokenize,\n",
    "                          smooth: Optional[int] = 1,\n",
    "                          bp_strategy: str = \"closest\") -> float:\n",
    "    if weights is None:\n",
    "        weights = [1.0 / max_n] * max_n\n",
    "\n",
    "    cand = tokenizer(candidate)\n",
    "    refs = [tokenizer(r) for r in references]\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        match, total = modified_precision(cand, refs, n)\n",
    "        p = (match / total) if total > 0 else 0.0\n",
    "        precisions.append(p)\n",
    "\n",
    "    if smooth is not None:\n",
    "        precisions = chen_cherry_smoothing(precisions, method=smooth)\n",
    "\n",
    "    if any(p <= 0 for p in precisions):\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        geo_mean = math.exp(sum(w * math.log(p) for w, p in zip(weights, precisions)))\n",
    "\n",
    "    bp = brevity_penalty(len(cand), [len(r) for r in refs], strategy=bp_strategy)\n",
    "    return bp * geo_mean\n",
    "\n",
    "def compute_corpus_bleu(candidates: Sequence[str],\n",
    "                        list_of_references: Sequence[Sequence[str]],\n",
    "                        max_n: int = 4,\n",
    "                        weights: Optional[Sequence[float]] = None,\n",
    "                        tokenizer: Callable[[str], List[str]] = simple_tokenize,\n",
    "                        bp_strategy: str = \"closest\") -> float:\n",
    "    if weights is None:\n",
    "        weights = [1.0 / max_n] * max_n\n",
    "\n",
    "    total_match = [0] * max_n\n",
    "    total_count = [0] * max_n\n",
    "    cand_len_total = 0\n",
    "    ref_len_choices = []\n",
    "\n",
    "    for cand_str, refs_strs in zip(candidates, list_of_references):\n",
    "        cand = tokenizer(cand_str)\n",
    "        refs = [tokenizer(r) for r in refs_strs]\n",
    "\n",
    "        cand_len_total += len(cand)\n",
    "        if bp_strategy == \"shortest\":\n",
    "            ref_len_choices.append(min(len(r) for r in refs))\n",
    "        elif bp_strategy == \"average\":\n",
    "            ref_len_choices.append(sum(len(r) for r in refs) / len(refs))\n",
    "        else:\n",
    "            ref_len_choices.append(min((len(r) for r in refs),\n",
    "                                       key=lambda rl: (abs(rl - len(cand)), rl)))\n",
    "\n",
    "        for n in range(1, max_n + 1):\n",
    "            m, t = modified_precision(cand, refs, n)\n",
    "            total_match[n - 1] += m\n",
    "            total_count[n - 1] += t\n",
    "\n",
    "    precisions = [(m / t) if t > 0 else 0.0 for m, t in zip(total_match, total_count)]\n",
    "    precisions = [p if p > 0 else 1e-9 for p in precisions]\n",
    "\n",
    "    if any(p <= 0 for p in precisions):\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        geo_mean = math.exp(sum(w * math.log(p) for w, p in zip(weights, precisions)))\n",
    "\n",
    "    R = sum(ref_len_choices)\n",
    "    C = cand_len_total\n",
    "    bp = math.exp(1 - R / C) if C < R and C > 0 else (0.0 if C == 0 else 1.0)\n",
    "\n",
    "    return bp * geo_mean\n",
    "\n",
    "# ---------------- Examples & Table ---------------- #\n",
    "rows = []\n",
    "\n",
    "def add_row(name, score, **kwargs):\n",
    "    rows.append({\"Example\": name, \"BLEU\": score, **kwargs})\n",
    "\n",
    "# 1) Simple BLEU-1\n",
    "candidate = \"The cat sat on the mat\"\n",
    "references = [\"The cat is on the mat\"]\n",
    "score_bleu1 = compute_sentence_bleu(candidate, references, max_n=1, weights=[1.0], smooth=None)\n",
    "add_row(\"Sentence BLEU-1 (no smoothing)\", score_bleu1, candidate=candidate, references=references)\n",
    "\n",
    "# 2) Short sentence BLEU-4\n",
    "candidate2 = \"hello\"\n",
    "references2 = [\"hello world\"]\n",
    "score_no_smooth = compute_sentence_bleu(candidate2, references2, max_n=4, smooth=None)\n",
    "score_smooth = compute_sentence_bleu(candidate2, references2, max_n=4, smooth=1)\n",
    "add_row(\"Short sentence BLEU-4 (no smoothing)\", score_no_smooth, candidate=candidate2, references=references2)\n",
    "add_row(\"Short sentence BLEU-4 (with smoothing=1)\", score_smooth, candidate=candidate2, references=references2)\n",
    "\n",
    "# 3) Multiple references\n",
    "candidate3 = \"the cat is on mat\"\n",
    "references3 = [\"the cat is on the mat\", \"there is a cat on the mat\"]\n",
    "score_multi_ref = compute_sentence_bleu(candidate3, references3, max_n=4, smooth=1)\n",
    "add_row(\"Sentence BLEU-4 with multiple references\", score_multi_ref, candidate=candidate3, references=references3)\n",
    "\n",
    "# 4) Brevity penalty strategies\n",
    "candidate4 = \"the cat is on mat\"\n",
    "references4 = [\"the cat is definitely on the mat\"]\n",
    "for strategy in [\"closest\", \"shortest\", \"average\"]:\n",
    "    score = compute_sentence_bleu(candidate4, references4, max_n=4, smooth=1, bp_strategy=strategy)\n",
    "    add_row(f\"BP strategy: {strategy}\", score, candidate=candidate4, references=references4)\n",
    "\n",
    "# 5) Custom weights\n",
    "score_bleu2 = compute_sentence_bleu(candidate3, references3, max_n=2, weights=[0.5, 0.5], smooth=1)\n",
    "score_bleu4 = compute_sentence_bleu(candidate3, references3, max_n=4, weights=[0.25]*4, smooth=1)\n",
    "add_row(\"Custom weights BLEU-2\", score_bleu2, candidate=candidate3, references=references3)\n",
    "add_row(\"Custom weights BLEU-4\", score_bleu4, candidate=candidate3, references=references3)\n",
    "\n",
    "# 6) Corpus BLEU\n",
    "candidates = [\"the cat is on mat\", \"there is a dog in the park\", \"I like pizza\"]\n",
    "references_list = [\n",
    "    [\"the cat is on the mat\", \"there is a cat on the mat\"],\n",
    "    [\"there is a dog at the park\", \"a dog is in the park\"],\n",
    "    [\"I love pizza\", \"I like pizza a lot\"]\n",
    "]\n",
    "corpus_score = compute_corpus_bleu(candidates, references_list, max_n=4)\n",
    "add_row(\"Corpus BLEU-4 (3 sentences)\", corpus_score)\n",
    "\n",
    "# 7) Tokenization effect\n",
    "def case_sensitive_tokenize(s: str) -> List[str]:\n",
    "    return s.split()\n",
    "\n",
    "cand_cs = \"The Cat Is On The Mat\"\n",
    "refs_cs = [\"the cat is on the mat\"]\n",
    "score_case_insensitive = compute_sentence_bleu(cand_cs, refs_cs, tokenizer=simple_tokenize, max_n=4, smooth=1)\n",
    "score_case_sensitive = compute_sentence_bleu(cand_cs, refs_cs, tokenizer=case_sensitive_tokenize, max_n=4, smooth=1)\n",
    "add_row(\"Case-insensitive tokenizer\", score_case_insensitive, candidate=cand_cs, references=refs_cs)\n",
    "add_row(\"Case-sensitive tokenizer\", score_case_sensitive, candidate=cand_cs, references=refs_cs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7dc38",
   "metadata": {},
   "source": [
    "The above script:\n",
    "- Implements sentence- and corpus-level BLEU.\n",
    "- Runs several test scenarios (BLEU-1, BLEU-4 with/without smoothing, multiple refs, brevity penalties, custom weights, corpus BLEU, tokenization effects).\n",
    "- Collects everything into a pandas DataFrame for inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240535d5",
   "metadata": {},
   "source": [
    "# ROUGE: simple implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c58236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Precision: 0.83\n",
      "ROUGE-1 Recall: 0.71\n",
      "ROUGE-1 F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_rouge_1(candidate, reference):\n",
    "    # Tokenize the sentences\n",
    "    candidate_tokens = candidate.split()\n",
    "    reference_tokens = reference.split()\n",
    "\n",
    "    # Count unigrams\n",
    "    candidate_counts = Counter(candidate_tokens)\n",
    "    reference_counts = Counter(reference_tokens)\n",
    "\n",
    "    # Calculate overlapping unigrams\n",
    "    overlapping_unigrams = sum(min(candidate_counts[word], reference_counts[word]) for word in candidate_counts)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = overlapping_unigrams / max(1, len(reference_tokens))\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = overlapping_unigrams / max(1, len(candidate_tokens))\n",
    "\n",
    "    # Calculate F1-score\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "candidate = \"the cat sat on the mat\"\n",
    "reference = \"the cat is sitting on the mat\"\n",
    "\n",
    "scores = compute_rouge_1(candidate, reference)\n",
    "print(f\"ROUGE-1 Precision: {scores['precision']:.2f}\")\n",
    "print(f\"ROUGE-1 Recall: {scores['recall']:.2f}\")\n",
    "print(f\"ROUGE-1 F1 Score: {scores['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de2628",
   "metadata": {},
   "source": [
    "# Extension to ROUGE-2, and ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d55e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: {'precision': 0.8, 'recall': 1.0, 'f1': 0.888888888888889}\n",
      "ROUGE-2: {'precision': 0.75, 'recall': 1.0, 'f1': 0.8571428571428571}\n",
      "ROUGE-L: {'precision': 0.8, 'recall': 1.0, 'f1': 0.888888888888889}\n",
      "\n",
      "(Average over refs)\n",
      "ROUGE-1: {'precision': 0.8, 'recall': 0.8333333333333333, 'f1': 0.8080808080808081}\n",
      "ROUGE-2: {'precision': 0.5, 'recall': 0.6, 'f1': 0.5396825396825397}\n",
      "ROUGE-L: {'precision': 0.6000000000000001, 'recall': 0.6666666666666666, 'f1': 0.6262626262626263}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Iterable, Sequence, Optional\n",
    "\n",
    "# ---------------- Tokenization ---------------- #\n",
    "def tokenize(s: str, lowercase: bool = True, strip_punct: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    if strip_punct:\n",
    "        s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    return s.split()\n",
    "\n",
    "def ngrams_from_tokens(tokens: Sequence[str], n: int) -> Iterable[Tuple[str, ...]]:\n",
    "    L = len(tokens)\n",
    "    for i in range(max(0, L - n + 1)):\n",
    "        yield tuple(tokens[i : i + n])\n",
    "\n",
    "# ---------------- BLEU (existing helpers, optional) ---------------- #\n",
    "def modified_precision(candidate_tokens: List[str],\n",
    "                       references_tokens: List[List[str]],\n",
    "                       n: int) -> Tuple[int, int]:\n",
    "    cand_counts = Counter(ngrams_from_tokens(candidate_tokens, n))\n",
    "    if not cand_counts:\n",
    "        return 0, 0\n",
    "\n",
    "    max_ref_counts = Counter()\n",
    "    for ref in references_tokens:\n",
    "        ref_counts = Counter(ngrams_from_tokens(ref, n))\n",
    "        for g, c in ref_counts.items():\n",
    "            if c > max_ref_counts[g]:\n",
    "                max_ref_counts[g] = c\n",
    "\n",
    "    clipped = {g: min(c, max_ref_counts.get(g, 0)) for g, c in cand_counts.items()}\n",
    "    return sum(clipped.values()), sum(cand_counts.values())\n",
    "\n",
    "# ---------------- ROUGE-N (N = 1 or 2) ---------------- #\n",
    "def _rouge_n_pair(candidate_tokens: List[str], reference_tokens: List[str], n: int) -> Dict[str, float]:\n",
    "    cand_counts = Counter(ngrams_from_tokens(candidate_tokens, n))\n",
    "    ref_counts  = Counter(ngrams_from_tokens(reference_tokens, n))\n",
    "\n",
    "    if not ref_counts and not cand_counts:\n",
    "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}  # both empty\n",
    "    if not ref_counts:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    if not cand_counts:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    overlap = 0\n",
    "    for g, c in cand_counts.items():\n",
    "        overlap += min(c, ref_counts.get(g, 0))\n",
    "\n",
    "    cand_total = sum(cand_counts.values())\n",
    "    ref_total  = sum(ref_counts.values())\n",
    "\n",
    "    precision = overlap / cand_total if cand_total > 0 else 0.0\n",
    "    recall    = overlap / ref_total  if ref_total  > 0 else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def compute_rouge_n(candidate: str,\n",
    "                    references: Sequence[str],\n",
    "                    n: int = 1,\n",
    "                    aggregator: str = \"max\",\n",
    "                    lowercase: bool = True,\n",
    "                    strip_punct: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    ROUGE-N over multiple references.\n",
    "    aggregator='max' -> take the reference with highest F1\n",
    "    aggregator='avg' -> average precision/recall/F1 over references\n",
    "    \"\"\"\n",
    "    cand_tok = tokenize(candidate, lowercase=lowercase, strip_punct=strip_punct)\n",
    "    ref_toks = [tokenize(r, lowercase=lowercase, strip_punct=strip_punct) for r in references]\n",
    "\n",
    "    scores = [_rouge_n_pair(cand_tok, rt, n) for rt in ref_toks]\n",
    "    if not scores:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    if aggregator == \"avg\":\n",
    "        p = sum(s[\"precision\"] for s in scores) / len(scores)\n",
    "        r = sum(s[\"recall\"]    for s in scores) / len(scores)\n",
    "        f = sum(s[\"f1\"]        for s in scores) / len(scores)\n",
    "        return {\"precision\": p, \"recall\": r, \"f1\": f}\n",
    "    else:  # 'max'\n",
    "        best = max(scores, key=lambda s: s[\"f1\"])\n",
    "        return best\n",
    "\n",
    "# ---------------- ROUGE-L (Longest Common Subsequence) ---------------- #\n",
    "def _lcs_length(a: List[str], b: List[str]) -> int:\n",
    "    # Classic DP for LCS length, O(len(a)*len(b)) time, O(min) space possible; here simple 2D for clarity.\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
    "    return dp[m][n]\n",
    "\n",
    "# using recursing approach for longest common sequence problem\n",
    "def _lcs_length_recursive_dp(a,b):\n",
    "    m, n = len(a), len(b)\n",
    "    def dp(i,j):\n",
    "        if i==m or j == n:\n",
    "            return 0\n",
    "        return 1+ dp(i+1,j+1) if a[i]==b[j] else max(dp(i+1,j), dp(i,j+1))\n",
    "    return dp(0,0)\n",
    "\n",
    "\n",
    "def _rouge_l_pair(candidate_tokens: List[str], reference_tokens: List[str]) -> Dict[str, float]:\n",
    "    if not candidate_tokens and not reference_tokens:\n",
    "        return {\"precision\": 1.0, \"recall\": 1.0, \"f1\": 1.0}\n",
    "    if not reference_tokens or not candidate_tokens:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    lcs = _lcs_length(candidate_tokens, reference_tokens)\n",
    "    # lcs = _lcs_length_recursive_dp(candidate_tokens, reference_tokens)\n",
    "    p = lcs / len(candidate_tokens) if len(candidate_tokens) > 0 else 0.0\n",
    "    r = lcs / len(reference_tokens) if len(reference_tokens) > 0 else 0.0\n",
    "    f1 = (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def compute_rouge_l(candidate: str,\n",
    "                    references: Sequence[str],\n",
    "                    aggregator: str = \"max\",\n",
    "                    lowercase: bool = True,\n",
    "                    strip_punct: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    ROUGE-L over multiple references.\n",
    "    aggregator='max' -> take the reference with highest F1\n",
    "    aggregator='avg' -> average precision/recall/F1 over references\n",
    "    \"\"\"\n",
    "    cand_tok = tokenize(candidate, lowercase=lowercase, strip_punct=strip_punct)\n",
    "    ref_toks = [tokenize(r, lowercase=lowercase, strip_punct=strip_punct) for r in references]\n",
    "\n",
    "    scores = [_rouge_l_pair(cand_tok, rt) for rt in ref_toks]\n",
    "    if not scores:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    if aggregator == \"avg\":\n",
    "        p = sum(s[\"precision\"] for s in scores) / len(scores)\n",
    "        r = sum(s[\"recall\"]    for s in scores) / len(scores)\n",
    "        f = sum(s[\"f1\"]        for s in scores) / len(scores)\n",
    "        return {\"precision\": p, \"recall\": r, \"f1\": f}\n",
    "    else:  # 'max'\n",
    "        best = max(scores, key=lambda s: s[\"f1\"])\n",
    "        return best\n",
    "\n",
    "# ---------------- Convenience wrapper: compute all ---------------- #\n",
    "def compute_rouge_all(candidate: str,\n",
    "                      references: Sequence[str],\n",
    "                      aggregator: str = \"max\",\n",
    "                      lowercase: bool = True,\n",
    "                      strip_punct: bool = True) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"rouge1\": {\"precision\":..., \"recall\":..., \"f1\":...},\n",
    "        \"rouge2\": {\"precision\":..., \"recall\":..., \"f1\":...},\n",
    "        \"rougeL\": {\"precision\":..., \"recall\":..., \"f1\":...}\n",
    "      }\n",
    "    \"\"\"\n",
    "    r1 = compute_rouge_n(candidate, references, n=1, aggregator=aggregator,\n",
    "                         lowercase=lowercase, strip_punct=strip_punct)\n",
    "    r2 = compute_rouge_n(candidate, references, n=2, aggregator=aggregator,\n",
    "                         lowercase=lowercase, strip_punct=strip_punct)\n",
    "    rL = compute_rouge_l(candidate, references, aggregator=aggregator,\n",
    "                         lowercase=lowercase, strip_punct=strip_punct)\n",
    "    return {\"rouge1\": r1, \"rouge2\": r2, \"rougeL\": rL}\n",
    "\n",
    "# ---------------- Example usage ---------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    cand = \"Alice won the contest yesterday\"\n",
    "    refs = [\n",
    "        \"Alice won the contest\",\n",
    "        \"The contest was won by Alice\"\n",
    "    ]\n",
    "\n",
    "    scores = compute_rouge_all(cand, refs, aggregator=\"max\")\n",
    "    print(\"ROUGE-1:\", scores[\"rouge1\"])\n",
    "    print(\"ROUGE-2:\", scores[\"rouge2\"])\n",
    "    print(\"ROUGE-L:\", scores[\"rougeL\"])\n",
    "\n",
    "    # Average over references instead of max:\n",
    "    scores_avg = compute_rouge_all(cand, refs, aggregator=\"avg\")\n",
    "    print(\"\\n(Average over refs)\")\n",
    "    print(\"ROUGE-1:\", scores_avg[\"rouge1\"])\n",
    "    print(\"ROUGE-2:\", scores_avg[\"rouge2\"])\n",
    "    print(\"ROUGE-L:\", scores_avg[\"rougeL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf4268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b0107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
