{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44beee2",
   "metadata": {},
   "source": [
    "# BPE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a061d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word freq:  Counter({'low_': 5, 'newest_': 5, 'lower_': 1, 'widest_': 1})\n",
      "word tokens:  {'low_': ['l', 'o', 'w', '_'], 'lower_': ['l', 'o', 'w', 'e', 'r', '_'], 'newest_': ['n', 'e', 'w', 'e', 's', 't', '_'], 'widest_': ['w', 'i', 'd', 'e', 's', 't', '_']}\n",
      "pair counts:  Counter({('l', 'o'): 6, ('o', 'w'): 6, ('w', 'e'): 6, ('e', 's'): 6, ('s', 't'): 6, ('t', '_'): 6, ('w', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1})\n",
      "pair counts:  Counter({('lo', 'w'): 6, ('w', 'e'): 6, ('e', 's'): 6, ('s', 't'): 6, ('t', '_'): 6, ('w', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1})\n",
      "pair counts:  Counter({('e', 's'): 6, ('s', 't'): 6, ('t', '_'): 6, ('low', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('w', 'e'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1})\n",
      "pair counts:  Counter({('es', 't'): 6, ('t', '_'): 6, ('low', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('w', 'es'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'es'): 1})\n",
      "pair counts:  Counter({('est', '_'): 6, ('low', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('w', 'est'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est'): 1})\n",
      "pair counts:  Counter({('low', '_'): 5, ('n', 'e'): 5, ('e', 'w'): 5, ('w', 'est_'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est_'): 1})\n",
      "pair counts:  Counter({('n', 'e'): 5, ('e', 'w'): 5, ('w', 'est_'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est_'): 1})\n",
      "pair counts:  Counter({('ne', 'w'): 5, ('w', 'est_'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est_'): 1})\n",
      "pair counts:  Counter({('new', 'est_'): 5, ('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est_'): 1})\n",
      "pair counts:  Counter({('low', 'e'): 1, ('e', 'r'): 1, ('r', '_'): 1, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'est_'): 1})\n",
      "new word tokens:  {'low_': ['low_'], 'lower_': ['low', 'e', 'r', '_'], 'newest_': ['newest_'], 'widest_': ['w', 'i', 'd', 'est_']}\n",
      "Vocabulary: {'_': 0, 'd': 1, 'e': 2, 'i': 3, 'l': 4, 'n': 5, 'o': 6, 'r': 7, 's': 8, 't': 9, 'w': 10, 'lo': 11, 'low': 12, 'es': 13, 'est': 14, 'est_': 15, 'low_': 16, 'ne': 17, 'new': 18, 'newest_': 19}\n",
      "Merge rules: [('l', 'o'), ('lo', 'w'), ('e', 's'), ('es', 't'), ('est', '_'), ('low', '_'), ('n', 'e'), ('ne', 'w'), ('new', 'est_')]\n",
      "Encoded 'lowest': [12, 15]\n",
      "Decoded text: lowest\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=100, min_frequency=2):\n",
    "        # Initialize the tokenizer with target vocab size and minimum pair frequency\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.merges = []       # list to store merge rules as tuples (token_a, token_b)\n",
    "        self.token2id = {}     # dictionary mapping token string -> token ID\n",
    "        self.id2token = {}     # dictionary mapping token ID -> token string\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Training: learn BPE merges from the given corpus text.\n",
    "        # corpus can be a single string or a list of strings.\n",
    "        if isinstance(corpus, str):\n",
    "            words = corpus.split()\n",
    "        else:\n",
    "            words = []\n",
    "            for text in corpus:\n",
    "                words.extend(text.split())\n",
    "        # Append '_' marker to each word to mark end-of-word\n",
    "        words = [word + '_' for word in words]\n",
    "        # Count frequency of each distinct word in the corpus\n",
    "        word_freq = Counter(words)\n",
    "        print(\"Word freq: \",word_freq)\n",
    "        # Initialize base vocabulary with all unique characters (including '_')\n",
    "        base_tokens = set()\n",
    "        for word in word_freq:\n",
    "            base_tokens.update(list(word))\n",
    "        base_tokens = sorted(base_tokens)  # sort for consistency (optional)\n",
    "        # Assign an ID to each base token (character)\n",
    "        self.token2id = {token: idx for idx, token in enumerate(base_tokens)}\n",
    "        self.id2token = {idx: token for token, idx in self.token2id.items()}\n",
    "        # Represent each word as a list of character tokens (with '_')\n",
    "        word_tokens = {word: list(word) for word in word_freq}\n",
    "        print(\"word tokens: \",word_tokens)\n",
    "        # Learn merge rules until vocab size reached or no frequent pair meets threshold\n",
    "        while len(self.token2id) < self.vocab_size:\n",
    "            # Count frequency of each adjacent token pair across all words\n",
    "            pair_counts = Counter()\n",
    "            for word, freq in word_freq.items():\n",
    "                tokens = word_tokens[word]\n",
    "                # count pairs in this word's token sequence\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    pair = (tokens[i], tokens[i+1])\n",
    "                    pair_counts[pair] += freq\n",
    "            print(\"pair counts: \", pair_counts)\n",
    "            if not pair_counts:\n",
    "                break  # no pairs to merge (shouldn't really happen unless corpus is empty)\n",
    "            # Find the most frequent pair\n",
    "            (token_a, token_b), pair_freq = pair_counts.most_common(1)[0]\n",
    "            if pair_freq < self.min_frequency:\n",
    "                break  # stop if no pair is frequent enough\n",
    "            # Merge this pair into a new token\n",
    "            new_token = token_a + token_b\n",
    "            # Add new token to vocab with the next available ID\n",
    "            new_id = len(self.token2id)\n",
    "            self.token2id[new_token] = new_id\n",
    "            self.id2token[new_id] = new_token\n",
    "            # Record the merge rule\n",
    "            self.merges.append((token_a, token_b))\n",
    "            # Update word token sequences: replace occurrences of the pair with the new token\n",
    "            for word, tokens in word_tokens.items():\n",
    "                i = 0\n",
    "                new_tokens = []\n",
    "                while i < len(tokens) - 1:\n",
    "                    if tokens[i] == token_a and tokens[i+1] == token_b:\n",
    "                        # Merge token_a and token_b into new_token\n",
    "                        new_tokens.append(new_token)\n",
    "                        i += 2  # skip over the merged pair\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                # Don't forget the last token if it wasn't part of a merge\n",
    "                if i < len(tokens):\n",
    "                    new_tokens.append(tokens[i])\n",
    "                # Update the word's token list\n",
    "                word_tokens[word] = new_tokens\n",
    "        print(\"new word tokens: \", word_tokens)\n",
    "            # Continue to next merge iteration\n",
    "        # Training complete. We have our merges and vocab.\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert a text string into a list of token IDs using learned merges.\n",
    "        tokens_ids = []\n",
    "        # Split input text into words and encode each word\n",
    "        for word in text.split():\n",
    "            # Start with characters + '_' for the word\n",
    "            tokens = list(word) + ['_']\n",
    "            # Apply each merge rule in order to the token list\n",
    "            for token_a, token_b in self.merges:\n",
    "                i = 0\n",
    "                merged_tokens = []\n",
    "                while i < len(tokens) - 1:\n",
    "                    if tokens[i] == token_a and tokens[i+1] == token_b:\n",
    "                        merged_tokens.append(token_a + token_b)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        merged_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                if i < len(tokens):\n",
    "                    merged_tokens.append(tokens[i])\n",
    "                tokens = merged_tokens\n",
    "            # Convert tokens to IDs\n",
    "            for token in tokens:\n",
    "                # .get(token) will fetch the ID; all tokens should exist from training\n",
    "                tokens_ids.append(self.token2id.get(token, None))\n",
    "        return tokens_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # Convert a list of token IDs back into the original text string.\n",
    "        tokens = [self.id2token[token_id] for token_id in token_ids]\n",
    "        text = \"\"\n",
    "        for token in tokens:\n",
    "            if token.endswith('_'):\n",
    "                # Remove the end-of-word marker and add a space\n",
    "                text += token[:-1] + \" \"\n",
    "            else:\n",
    "                # Token without marker (should be punctuation or part of word that isn't ending)\n",
    "                text += token\n",
    "        return text.strip()  # strip any trailing space\n",
    "\n",
    "# Sample corpus for training\n",
    "corpus = [\n",
    "    \"low\", \"lower\", \"newest\", \"widest\",\n",
    "    \"low\", \"low\", \"low\", \"low\",  # Repetition to build frequency\n",
    "    \"newest\", \"newest\", \"newest\", \"newest\"\n",
    "]\n",
    "\n",
    "# Initialize tokenizer with small vocab size\n",
    "tokenizer = BPETokenizer(vocab_size=50, min_frequency=2)\n",
    "tokenizer.train(corpus)\n",
    "\n",
    "# Print learned vocabulary\n",
    "print(\"Vocabulary:\", tokenizer.token2id)\n",
    "print(\"Merge rules:\", tokenizer.merges)\n",
    "\n",
    "# Encode a new word\n",
    "encoded = tokenizer.encode(\"lowest\")\n",
    "print(\"Encoded 'lowest':\", encoded)\n",
    "\n",
    "# Decode the tokens back\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8346c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in ./.venv/lib/python3.9/site-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.9/site-packages (from tokenizers) (0.34.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d881c",
   "metadata": {},
   "source": [
    "# Train and use HuggingFace’s ByteLevelBPETokenizer from the tokenizers library.\n",
    "- Trains a Byte-Level BPE tokenizer on a small text file.\n",
    "- Saves the tokenizer files.\n",
    "- Loads the tokenizer and uses it to encode/decode a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cf6a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['Hello', ',', 'ĠB', 'y', 'te', 'L', 'e', 'v', 'el', 'ĠB', 'P', 'E', '!']\n",
      "Token IDs: [271, 16, 268, 93, 267, 48, 73, 90, 261, 268, 52, 41, 5]\n",
      "Decoded: Hello, ByteLevel BPE!\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# 1. Train a Byte-Level BPE tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Train on a simple sample text file\n",
    "tokenizer.train(files=\"sample.txt\", vocab_size=1000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_model(\"tokenizer_model\")\n",
    "\n",
    "# 2. Load the trained tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"tokenizer_model/vocab.json\",\n",
    "    \"tokenizer_model/merges.txt\"\n",
    ")\n",
    "\n",
    "# 3. Encode and decode a sentence\n",
    "encoded = tokenizer.encode(\"Hello, ByteLevel BPE!\")\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0579188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.55.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.9/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.9/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a54095",
   "metadata": {},
   "source": [
    "# use a pretrained Byte-Level BPE tokenizer (GPT-2 from Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ecad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 11, 30589, 4971, 347, 11401, 0]\n",
      "Decoded: Hello, ByteLevel BPE!\n",
      "Tokens: ['Hello', ',', 'ĠByte', 'Level', 'ĠB', 'PE', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load the pretrained GPT-2 tokenizer (ByteLevel BPE)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, ByteLevel BPE!\"\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", encoded)\n",
    "\n",
    "# Decode the token IDs\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "# Optional: Show tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6e63b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
