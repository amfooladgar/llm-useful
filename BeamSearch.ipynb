{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9780f1",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41655b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: <START> hello foo <EOS>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "def beam_search(start_token: str,\n",
    "                get_next_token_probs,\n",
    "                beam_width: int = 3,\n",
    "                max_length: int = 10) -> List[str]:\n",
    "    # Each beam entry is a tuple: (sequence, total log-prob)\n",
    "    beam = [([start_token], 0.0)]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, seq_logprob in beam:\n",
    "            if seq[-1] == EOS_TOKEN:\n",
    "                # Already ended â€” carry forward unchanged\n",
    "                all_candidates.append((seq, seq_logprob))\n",
    "                continue\n",
    "\n",
    "            # Get top next-token candidates and their log-probs\n",
    "            next_tokens = get_next_token_probs(seq)\n",
    "\n",
    "            for token, log_prob in next_tokens:\n",
    "                new_seq = seq + [token]\n",
    "                new_logprob = seq_logprob + log_prob\n",
    "                all_candidates.append((new_seq, new_logprob))\n",
    "\n",
    "        # Keep the top-k sequences with highest log-prob\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beam = all_candidates[:beam_width]\n",
    "\n",
    "        # Stop early if all beams have ended\n",
    "        if all(seq[-1] == EOS_TOKEN for seq, _ in beam):\n",
    "            break\n",
    "\n",
    "    best_sequence, _ = beam[0]\n",
    "    return best_sequence\n",
    "\n",
    "def get_next_token_probs(seq: List[str]) -> List[Tuple[str, float]]:\n",
    "    vocab = [\"hello\", \"world\", \"foo\", \"bar\", \"<EOS>\"]\n",
    "    return [(token, math.log(random.uniform(0.1, 1.0))) for token in vocab]\n",
    "\n",
    "output = beam_search(\"<START>\", get_next_token_probs, beam_width=3, max_length=5)\n",
    "print(\"Generated sequence:\", \" \".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psudo code for interview preparation\n",
    "# Length normalization:\n",
    "# Divide score by (len(seq) ** alpha) before sorting.\n",
    "\n",
    "# Constrained decoding:\n",
    "# Filter probs to zero-out tokens that violate constraints.\n",
    "\n",
    "# GPU batch optimization:\n",
    "# Stack all beams into one tensor and run a single forward pass.\n",
    "\n",
    "def beam_search(start_token, beam_width, max_len):\n",
    "    beams = [( [start_token], 0.0 )]  # (tokens, log_prob)\n",
    "    for _ in range(max_len):\n",
    "        candidates = []\n",
    "        for seq, score in beams:\n",
    "            probs = next_token_probs(seq)  # returns log probs\n",
    "            for tok, logp in probs.topk(beam_width):\n",
    "                candidates.append((seq + [tok], score + logp))\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "    return beams[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827f588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
