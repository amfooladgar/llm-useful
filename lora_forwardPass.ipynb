{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac786429",
   "metadata": {},
   "source": [
    "# implement LoRA for a linear layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be315d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, orig_linear: nn.Linear, r: int = 8, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Wraps a given linear layer with LoRA adaptation.\n",
    "        orig_linear: an existing nn.Linear layer from a pre-trained model (weights frozen).\n",
    "        r: rank of the LoRA adapters.\n",
    "        alpha: scaling factor for LoRA (often set such that alpha/r is 1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = orig_linear.in_features\n",
    "        self.out_features = orig_linear.out_features\n",
    "        # Freeze original weight and bias\n",
    "        self.weight = nn.Parameter(orig_linear.weight.data, requires_grad=False)\n",
    "        if orig_linear.bias is not None:\n",
    "            self.bias = nn.Parameter(orig_linear.bias.data, requires_grad=False)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # LoRA low-rank matrices\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # \"Down\" projection: reduces dimension from in_features to r\n",
    "        self.lora_down = nn.Parameter(torch.zeros((r, self.in_features)))\n",
    "        # \"Up\" projection: increases dimension from r to out_features\n",
    "        self.lora_up   = nn.Parameter(torch.zeros((self.out_features, r)))\n",
    "        # Initialize LoRA weights: usually lora_down random, lora_up zero\n",
    "        nn.init.kaiming_uniform_(self.lora_down, a=math.sqrt(5))  # He init for down-proj\n",
    "        nn.init.zeros_(self.lora_up)  # start with no effect\n",
    "        # Note: starting with lora_up = 0 means initially the LoRA doesn't change the output\n",
    "        # (since lora_down * 0 = 0), so the model starts exactly like the pre-trained one.\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute base linear output (no grad, since weight is frozen)\n",
    "        # x shape: [batch, in_features]\n",
    "        # weight shape: [out_features, in_features]\n",
    "        result = torch.matmul(x, self.weight.T)\n",
    "        # Compute LoRA adaptation: (x * lora_down^T) * lora_up^T scaled by alpha/r\n",
    "        # lora_down^T shape: [in_features, r], lora_up^T: [r, out_features]\n",
    "        lora_out = x @ self.lora_down.T    # shape: [batch, r]\n",
    "        lora_out = lora_out @ self.lora_up.T  # shape: [batch, out_features]\n",
    "        # Scale the LoRA output\n",
    "        result += lora_out * (self.alpha / self.r)\n",
    "        # Add bias if present\n",
    "        if self.bias is not None:\n",
    "            result += self.bias\n",
    "        return result\n",
    "\n",
    "# Example usage:\n",
    "# Suppose we have a GPT-2 model and want to apply LoRA to its first fully-connected layer\n",
    "from transformers import GPT2Model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "# Pick a linear layer from the model, e.g., the feed-forward layer in the first Transformer block\n",
    "orig_linear = model.h[0].mlp.c_fc  # (Assume c_fc is a nn.Linear in GPT2 block 0)\n",
    "# Replace it with a LoRA-wrapped layer\n",
    "model.h[0].mlp.c_fc = LoRALinear(orig_linear, r=8, alpha=8)\n",
    "# Now model.h[0].mlp.c_fc will only train the LoRA params. Freeze others as needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
