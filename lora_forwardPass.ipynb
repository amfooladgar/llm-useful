{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac786429",
   "metadata": {},
   "source": [
    "# implement LoRA for a linear layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be315d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv1D' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m orig_linear = model.h[\u001b[32m0\u001b[39m].mlp.c_fc  \u001b[38;5;66;03m# (Assume c_fc is a nn.Linear in GPT2 block 0)\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Replace it with a LoRA-wrapped layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m model.h[\u001b[32m0\u001b[39m].mlp.c_fc = \u001b[43mLoRALinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_linear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Now model.h[0].mlp.c_fc will only train the LoRA params. Freeze others as needed.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mLoRALinear.__init__\u001b[39m\u001b[34m(self, orig_linear, r, alpha)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mWraps a given linear layer with LoRA adaptation.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03morig_linear: an existing nn.Linear layer from a pre-trained model (weights frozen).\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mr: rank of the LoRA adapters.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03malpha: scaling factor for LoRA (often set such that alpha/r is 1).\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = \u001b[43morig_linear\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_features\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = orig_linear.out_features\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Freeze original weight and bias\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/llm-useful/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Conv1D' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, orig_linear: nn.Linear, r: int = 8, alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        Wraps a given linear layer with LoRA adaptation.\n",
    "        orig_linear: an existing nn.Linear layer from a pre-trained model (weights frozen).\n",
    "        r: rank of the LoRA adapters.\n",
    "        alpha: scaling factor for LoRA (often set such that alpha/r is 1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = orig_linear.in_features\n",
    "        self.out_features = orig_linear.out_features\n",
    "        # Freeze original weight and bias\n",
    "        self.weight = nn.Parameter(orig_linear.weight.data, requires_grad=False)\n",
    "        if orig_linear.bias is not None:\n",
    "            self.bias = nn.Parameter(orig_linear.bias.data, requires_grad=False)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # LoRA low-rank matrices\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # \"Down\" projection: reduces dimension from in_features to r\n",
    "        self.lora_down = nn.Parameter(torch.zeros((r, self.in_features)))\n",
    "        # \"Up\" projection: increases dimension from r to out_features\n",
    "        self.lora_up   = nn.Parameter(torch.zeros((self.out_features, r)))\n",
    "        # Initialize LoRA weights: usually lora_down random, lora_up zero\n",
    "        nn.init.kaiming_uniform_(self.lora_down, a=math.sqrt(5))  # He init for down-proj\n",
    "        nn.init.zeros_(self.lora_up)  # start with no effect\n",
    "        # Note: starting with lora_up = 0 means initially the LoRA doesn't change the output\n",
    "        # (since lora_down * 0 = 0), so the model starts exactly like the pre-trained one.\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute base linear output (no grad, since weight is frozen)\n",
    "        # x shape: [batch, in_features]\n",
    "        # weight shape: [out_features, in_features]\n",
    "        result = torch.matmul(x, self.weight.T)\n",
    "        # Compute LoRA adaptation: (x * lora_down^T) * lora_up^T scaled by alpha/r\n",
    "        # lora_down^T shape: [in_features, r], lora_up^T: [r, out_features]\n",
    "        lora_out = x @ self.lora_down.T    # shape: [batch, r]\n",
    "        lora_out = lora_out @ self.lora_up.T  # shape: [batch, out_features]\n",
    "        # Scale the LoRA output\n",
    "        result += lora_out * (self.alpha / self.r)\n",
    "        # Add bias if present\n",
    "        if self.bias is not None:\n",
    "            result += self.bias\n",
    "        return result\n",
    "\n",
    "# Example usage:\n",
    "# Suppose we have a GPT-2 model and want to apply LoRA to its first fully-connected layer\n",
    "from transformers import GPT2Model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "# Pick a linear layer from the model, e.g., the feed-forward layer in the first Transformer block\n",
    "orig_linear = model.h[0].mlp.c_fc  # (Assume c_fc is a nn.Linear in GPT2 block 0)\n",
    "# Replace it with a LoRA-wrapped layer\n",
    "model.h[0].mlp.c_fc = LoRALinear(orig_linear, r=8, alpha=8)\n",
    "# Now model.h[0].mlp.c_fc will only train the LoRA params. Freeze others as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a0e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 884,736 || all params: 125,324,544 || trainable%: 0.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alifouladgar/Documents/llm-useful/.venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/alifouladgar/Documents/llm-useful/.venv/lib/python3.13/site-packages/peft/tuners/lora/layer.py:1803: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_fc\", \"c_proj\"]  # These are Conv1D layers in GPT-2\n",
    ")\n",
    "\n",
    "# Apply LoRA using PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Optional: See trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e311251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.5681\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Why is LoRA useful in large language models?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "print(f\"Loss: {outputs.loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c7b21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  GPT-2 with LoRA says:\n",
      "\n",
      "Why is LoRA useful in large language models?\n",
      "\n",
      "LOORA is used to represent an abstract class with a structure that is not bound by a reference to another class, such as a dictionary. LoRA is typically used in a type system like the Python language, and it is possible to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generation = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(generation[0], skip_special_tokens=True)\n",
    "print(\"ðŸ§  GPT-2 with LoRA says:\\n\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1ae98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
