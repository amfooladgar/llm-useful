{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d450ff",
   "metadata": {},
   "source": [
    "# cross-attention \n",
    "just to explain how query, key and value interact in cross-attention (implemented using numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c24fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (3, 16)\n",
      "Output: [[0.47424909 0.44825225 0.35960412 0.65028308 0.52966772 0.2177133\n",
      "  0.52158821 0.25817116 0.69138563 0.62032681 0.31718724 0.46422458\n",
      "  0.34938322 0.4858956  0.69959381 0.49094542]\n",
      " [0.48639217 0.42460981 0.32800314 0.63567388 0.52019432 0.23346849\n",
      "  0.54607276 0.27313906 0.66431665 0.61254396 0.30268994 0.48476288\n",
      "  0.33229508 0.51576049 0.69228928 0.47398688]\n",
      " [0.47873282 0.43109256 0.3423324  0.64023396 0.51838087 0.22245792\n",
      "  0.53300005 0.2641912  0.6771243  0.61896404 0.31349208 0.48526171\n",
      "  0.34509942 0.50194426 0.70135753 0.47967725]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    query: shape (num_queries, d_k)\n",
    "    key:   shape (num_keys, d_k)\n",
    "    value: shape (num_keys, d_v)\n",
    "    \n",
    "    returns: shape (num_queries, d_v)\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "\n",
    "    # Step 1: Compute raw attention scores (dot product between queries and keys)\n",
    "    scores = np.dot(query, key.T) / np.sqrt(d_k)  # shape: (num_queries, num_keys)\n",
    "\n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # stability\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)  # shape: (num_queries, num_keys)\n",
    "\n",
    "    # Step 3: Compute weighted sum of value vectors\n",
    "    output = np.dot(attention_weights, value)  # shape: (num_queries, d_v)\n",
    "\n",
    "    return output\n",
    "    \n",
    "# Toy example with 3 queries and 4 key-value pairs\n",
    "np.random.seed(42)\n",
    "Q = np.random.rand(3, 8)   # 3 query vectors (decoder tokens)\n",
    "K = np.random.rand(4, 8)   # 4 key vectors (encoder tokens)\n",
    "V = np.random.rand(4, 16)  # 4 value vectors with d_v = 16\n",
    "\n",
    "output = cross_attention(Q, K, V)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd651511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Summary tokens : ['cat', 'on', 'mat'] \n",
      "\n",
      "Attention weights (rows = summary queries, cols = document tokens):\n",
      "(query→) \\ (doc↓) | the   | cat   | sat   | on    | the   | mat  \n",
      "------------------+-------+-------+-------+-------+-------+------\n",
      "cat               | 0.161 | 0.240 | 0.146 | 0.146 | 0.161 | 0.146\n",
      "on                | 0.161 | 0.146 | 0.146 | 0.240 | 0.161 | 0.146\n",
      "mat               | 0.161 | 0.146 | 0.146 | 0.146 | 0.161 | 0.240\n",
      "\n",
      "Top for 'cat': doc token 'cat' (weight=0.240)\n",
      "\n",
      "Top for 'on': doc token 'on' (weight=0.240)\n",
      "\n",
      "Top for 'mat': doc token 'mat' (weight=0.240)\n",
      "\n",
      "Context vectors shape: (3, 6)\n",
      "[[ 0.05312044  0.31110953 -0.40936367 -0.26447013 -0.45491226  0.16061891]\n",
      " [-0.0312787   0.3201354  -0.35496194 -0.01554987 -0.45887702  0.18995572]\n",
      " [-0.16569856  0.20175229 -0.3578646  -0.09363573 -0.37566159  0.19032104]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    query: shape (num_queries, d_k)\n",
    "    key:   shape (num_keys, d_k)\n",
    "    value: shape (num_keys, d_v)\n",
    "    \n",
    "    returns: shape (num_queries, d_v)\n",
    "    \"\"\"\n",
    "    d_k = query.shape[-1]\n",
    "\n",
    "    # Step 1: Compute raw attention scores (dot product between queries and keys)\n",
    "    scores = np.dot(query, key.T) / np.sqrt(d_k)  # shape: (num_queries, num_keys)\n",
    "\n",
    "    # Step 2: Apply softmax to get attention weights\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # stability\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)  # shape: (num_queries, num_keys)\n",
    "\n",
    "    # Step 3: Compute weighted sum of value vectors\n",
    "    output = np.dot(attention_weights, value)  # shape: (num_queries, d_v)\n",
    "\n",
    "    return output, attention_weights, scores\n",
    "\n",
    "# ----------------------------\n",
    "# Word-labeled, interpretable setup\n",
    "# ----------------------------\n",
    "doc_tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]      # encoder tokens\n",
    "sum_tokens = [\"cat\", \"on\", \"mat\"]                           # decoder tokens (summary so far)\n",
    "\n",
    "# Tiny 4-D “semantic” basis so we get intuitive alignments:\n",
    "#   cat -> [1,0,0,0], sat -> [0,1,0,0], on -> [0,0,1,0], mat -> [0,0,0,1]\n",
    "#   the is a weak, uniform vector so it attracts less attention\n",
    "B = {\n",
    "    \"cat\": np.array([1.,0.,0.,0.]),\n",
    "    \"sat\": np.array([0.,1.,0.,0.]),\n",
    "    \"on\" : np.array([0.,0.,1.,0.]),\n",
    "    \"mat\": np.array([0.,0.,0.,1.]),\n",
    "    \"the\": np.array([0.2,0.2,0.2,0.2]),\n",
    "}\n",
    "\n",
    "# Keys & Values from the encoder (values can have different dim; here 6 for variety)\n",
    "d_k, d_v = 4, 6\n",
    "K = np.stack([B[w] for w in doc_tokens], axis=0)            # (num_keys=6, d_k=4)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "V = rng.normal(size=(len(doc_tokens), d_v)).astype(np.float32)  # random values carry content\n",
    "\n",
    "# Queries from the decoder (one per summary token)\n",
    "Q = np.stack([B[w] for w in sum_tokens], axis=0)            # (num_queries=3, d_k=4)\n",
    "\n",
    "# Run cross-attention\n",
    "context, attn, scores = cross_attention(Q, K, V)\n",
    "\n",
    "# ----------------------------\n",
    "# Pretty-print the attention map\n",
    "# ----------------------------\n",
    "def format_matrix(attn, row_labels, col_labels, precision=3):\n",
    "    # build a simple text table\n",
    "    col_head = [\"(query→) \\\\ (doc↓)\"] + col_labels\n",
    "    rows = []\n",
    "    for i, rlab in enumerate(row_labels):\n",
    "        row = [rlab] + [f\"{attn[i,j]:.{precision}f}\" for j in range(attn.shape[1])]\n",
    "        rows.append(row)\n",
    "    widths = [max(len(col_head[c]), max(len(r[c]) for r in rows)) for c in range(len(col_head))]\n",
    "    def fmt_line(cells):\n",
    "        return \" | \".join(cell.ljust(w) for cell, w in zip(cells, widths))\n",
    "    lines = [fmt_line(col_head),\n",
    "             \"-+-\".join(\"-\"*w for w in widths)]\n",
    "    lines += [fmt_line(r) for r in rows]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(\"Document tokens:\", doc_tokens)\n",
    "print(\"Summary tokens :\", sum_tokens, \"\\n\")\n",
    "\n",
    "print(\"Attention weights (rows = summary queries, cols = document tokens):\")\n",
    "print(format_matrix(attn, sum_tokens, doc_tokens))\n",
    "\n",
    "# Show the top attended doc token per summary token\n",
    "top_idx = attn.argmax(axis=1)\n",
    "for i, q in enumerate(sum_tokens):\n",
    "    print(f\"\\nTop for '{q}': doc token '{doc_tokens[top_idx[i]]}' \"\n",
    "          f\"(weight={attn[i, top_idx[i]]:.3f})\")\n",
    "\n",
    "# Context vectors that the decoder will use to predict the next summary tokens\n",
    "print(\"\\nContext vectors shape:\", context.shape)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc19aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:  the cat sat on the mat\n",
      "Summary (generated):  cat on cat <EOS>\n",
      "\n",
      "Cross-attention weights per step (rows = steps, cols = doc tokens):\n",
      "step  |    the |    cat |    sat |     on |    the |    mat\n",
      "-----------------------------------------------------------\n",
      "   1  |  0.170 |  0.167 |  0.156 |  0.174 |  0.170 |  0.162\n",
      "   2  |  0.170 |  0.168 |  0.163 |  0.165 |  0.170 |  0.164\n",
      "   3  |  0.164 |  0.167 |  0.166 |  0.170 |  0.164 |  0.169\n",
      "   4  |  0.170 |  0.168 |  0.163 |  0.165 |  0.170 |  0.164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# ----------------------------\n",
    "# Toy vocab & document\n",
    "# ----------------------------\n",
    "Vocab = [\"<BOS>\", \"<EOS>\", \"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "tok2id = {w:i for i,w in enumerate(Vocab)}\n",
    "id2tok = {i:w for w,i in tok2id.items()}\n",
    "doc_tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]  # source document\n",
    "\n",
    "# ----------------------------\n",
    "# Dimensions\n",
    "# ----------------------------\n",
    "d_model = 16\n",
    "d_k = 16\n",
    "d_v = 16\n",
    "\n",
    "# ----------------------------\n",
    "# Embeddings and projections\n",
    "# ----------------------------\n",
    "E_src = np.random.randn(len(Vocab), d_model) / np.sqrt(d_model)  # encoder token embeddings\n",
    "E_dec = np.random.randn(len(Vocab), d_model) / np.sqrt(d_model)  # decoder token embeddings\n",
    "W_k = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "W_v = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "W_q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "W_out = np.random.randn(d_v, len(Vocab)) / np.sqrt(d_v)\n",
    "b_out = np.zeros(len(Vocab))\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_attention(query, key, value):\n",
    "    # query: (d_k,), key: (Tsrc,d_k), value: (Tsrc,d_v)\n",
    "    scores = (key @ query) / np.sqrt(d_k)     # (Tsrc,)\n",
    "    attn = softmax(scores[None, :], axis=-1)[0]\n",
    "    ctx = attn @ value                        # (d_v,)\n",
    "    return ctx, attn, scores\n",
    "\n",
    "# ----------------------------\n",
    "# Encode the document (build K,V)\n",
    "# ----------------------------\n",
    "src_ids = [tok2id[w] for w in doc_tokens]\n",
    "src_emb = E_src[src_ids]            # (Tsrc, d_model)\n",
    "K = src_emb @ W_k                   # (Tsrc, d_k)\n",
    "V = src_emb @ W_v                   # (Tsrc, d_v)\n",
    "\n",
    "# ----------------------------\n",
    "# Decode autoregressively with EOS masked early\n",
    "# ----------------------------\n",
    "max_len = 6\n",
    "generated = []\n",
    "attn_history = []\n",
    "\n",
    "prev_id = tok2id[\"<BOS>\"]\n",
    "dec_state = E_dec[prev_id]\n",
    "\n",
    "content_ids = np.array([tok2id[w] for w in Vocab if w not in (\"<BOS>\", \"<EOS>\")])\n",
    "\n",
    "for t in range(max_len):\n",
    "    # 1) Make a query from the current decoder state\n",
    "    q = dec_state @ W_q             # (d_k,)\n",
    "\n",
    "    # 2) Cross-attend to the source\n",
    "    ctx, attn, scores = cross_attention(q, K, V)\n",
    "    attn_history.append(attn)\n",
    "\n",
    "    # 3) Predict next token from context\n",
    "    logits = ctx @ W_out + b_out\n",
    "\n",
    "    # --- Decoding constraints to avoid early stop / degeneracy ---\n",
    "    # disallow <BOS> always\n",
    "    logits[tok2id[\"<BOS>\"]] = -1e9\n",
    "    # disallow <EOS> for the first 3 steps\n",
    "    if t < 3:\n",
    "        logits[tok2id[\"<EOS>\"]] = -1e9\n",
    "    # (optional) discourage repeating the exact same token\n",
    "    if generated:\n",
    "        logits[generated[-1]] -= 2.0\n",
    "\n",
    "    probs = softmax(logits[None, :], axis=-1)[0]\n",
    "    next_id = int(np.argmax(probs))\n",
    "\n",
    "    generated.append(next_id)\n",
    "    if next_id == tok2id[\"<EOS>\"]:\n",
    "        break\n",
    "\n",
    "    # 4) Update decoder state (toy): combine next token embedding + context\n",
    "    dec_state = 0.5 * E_dec[next_id] + 0.5 * ctx\n",
    "\n",
    "# ----------------------------\n",
    "# Print results\n",
    "# ----------------------------\n",
    "gen_tokens = [id2tok[i] for i in generated]\n",
    "print(\"Document: \", \" \".join(doc_tokens))\n",
    "print(\"Summary (generated): \", \" \".join(gen_tokens))\n",
    "\n",
    "print(\"\\nCross-attention weights per step (rows = steps, cols = doc tokens):\")\n",
    "header = \"step  | \" + \" | \".join([f\"{w:>6}\" for w in doc_tokens])\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for t, a in enumerate(attn_history, start=1):\n",
    "    row = f\"{t:>4}  | \" + \" | \".join([f\"{w:>6.3f}\" for w in a])\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ccc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:  the cat sat on the mat\n",
      "Summary  :  cat sat on mat <EOS>\n",
      "\n",
      "Cross-attention weights per step (rows=steps, cols=document tokens):\n",
      "step  |    the |    cat |    sat |     on |    the |    mat\n",
      "-----------------------------------------------------------\n",
      "   1  |  0.161 |  0.240 |  0.146 |  0.146 |  0.161 |  0.146\n",
      "   2  |  0.161 |  0.146 |  0.240 |  0.146 |  0.161 |  0.146\n",
      "   3  |  0.161 |  0.146 |  0.146 |  0.240 |  0.161 |  0.146\n",
      "   4  |  0.161 |  0.146 |  0.146 |  0.146 |  0.161 |  0.240\n",
      "   5  |  0.161 |  0.146 |  0.146 |  0.146 |  0.161 |  0.240\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Vocab & document\n",
    "# ----------------------------\n",
    "Vocab = [\"<BOS>\", \"<EOS>\", \"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "tok2id = {w:i for i,w in enumerate(Vocab)}\n",
    "id2tok = {i:w for w,i in tok2id.items()}\n",
    "\n",
    "doc_tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]  # source document\n",
    "\n",
    "# ----------------------------\n",
    "# Dimensions\n",
    "# ----------------------------\n",
    "d_model = 5   # decoder \"state\" dim (one state per step)\n",
    "d_k = 4       # key/query dim\n",
    "d_v = 4       # value dim (same basis as content words)\n",
    "\n",
    "# ----------------------------\n",
    "# Semantic basis (orthogonal) for content words\n",
    "# ----------------------------\n",
    "e_cat = np.array([1.,0.,0.,0.])\n",
    "e_sat = np.array([0.,1.,0.,0.])\n",
    "e_on  = np.array([0.,0.,1.,0.])\n",
    "e_mat = np.array([0.,0.,0.,1.])\n",
    "e_the = np.array([0.2,0.2,0.2,0.2])  # weak, uniform\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder: keys/values from doc tokens\n",
    "# ----------------------------\n",
    "def enc_vec(w):\n",
    "    return {\"cat\": e_cat, \"sat\": e_sat, \"on\": e_on, \"mat\": e_mat}.get(w, e_the)\n",
    "\n",
    "K = np.stack([enc_vec(w) for w in doc_tokens], axis=0)  # (Tsrc, d_k)\n",
    "V = K.copy()                                            # (Tsrc, d_v)\n",
    "\n",
    "# ----------------------------\n",
    "# Decoder states & query projection\n",
    "# We design states so W_q maps:\n",
    "#   s0(<BOS>)->cat, s1(cat)->sat, s2(sat)->on, s3(on)->mat, s4(mat)->mat (to then produce EOS)\n",
    "# ----------------------------\n",
    "s0 = np.array([1,0,0,0,0], dtype=float)  # <BOS>\n",
    "s1 = np.array([0,1,0,0,0], dtype=float)  # after generating \"cat\"\n",
    "s2 = np.array([0,0,1,0,0], dtype=float)  # after \"sat\"\n",
    "s3 = np.array([0,0,0,1,0], dtype=float)  # after \"on\"\n",
    "s4 = np.array([0,0,0,0,1], dtype=float)  # after \"mat\"\n",
    "\n",
    "E_dec = {\n",
    "    \"<BOS>\": s0,\n",
    "    \"cat\":   s1,\n",
    "    \"sat\":   s2,\n",
    "    \"on\":    s3,\n",
    "    \"mat\":   s4,\n",
    "    \"<EOS>\": np.zeros_like(s0),\n",
    "    \"the\":   np.zeros_like(s0),\n",
    "}\n",
    "\n",
    "# W_q rows map state -> desired query\n",
    "# shape: (d_model, d_k). Row i is the query produced when that state is 1-hot.\n",
    "W_q = np.stack([\n",
    "    e_cat,  # from s0\n",
    "    e_sat,  # from s1\n",
    "    e_on,   # from s2\n",
    "    e_mat,  # from s3\n",
    "    e_mat,  # from s4 (we'll allow EOS at this step)\n",
    "], axis=0)  # (5,4)\n",
    "\n",
    "# ----------------------------\n",
    "# Output head: context -> logits over vocab\n",
    "# Make the next-token obvious:\n",
    "#   e_cat -> \"cat\", e_sat -> \"sat\", e_on -> \"on\", e_mat -> \"mat\"\n",
    "# Also: when EOS is allowed, make it beat \"mat\" if context = e_mat\n",
    "# ----------------------------\n",
    "W_out = np.zeros((d_v, len(Vocab)))\n",
    "# columns correspond to tokens; set them to the matching basis\n",
    "W_out[:, tok2id[\"cat\"]] = e_cat\n",
    "W_out[:, tok2id[\"sat\"]] = e_sat\n",
    "W_out[:, tok2id[\"on\"]]  = e_on\n",
    "W_out[:, tok2id[\"mat\"]] = e_mat\n",
    "# make EOS extra strong on e_mat so step after \"mat\" picks EOS\n",
    "W_out[:, tok2id[\"<EOS>\"]] = 2.0 * e_mat\n",
    "b_out = np.zeros(len(Vocab))\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_attention(query, key, value):\n",
    "    # query: (d_k,), key: (Tsrc,d_k), value: (Tsrc,d_v)\n",
    "    scores = (key @ query) / np.sqrt(d_k)          # (Tsrc,)\n",
    "    attn = softmax(scores[None, :], axis=-1)[0]    # (Tsrc,)\n",
    "    ctx = attn @ value                              # (d_v,)\n",
    "    return ctx, attn, scores\n",
    "\n",
    "# ----------------------------\n",
    "# Autoregressive decoding\n",
    "# ----------------------------\n",
    "max_len = 6\n",
    "generated = []\n",
    "attn_history = []\n",
    "\n",
    "state = E_dec[\"<BOS>\"]  # start\n",
    "for t in range(max_len):\n",
    "    # 1) make query from state\n",
    "    q = state @ W_q                       # (d_k,)\n",
    "\n",
    "    # 2) cross-attend to source\n",
    "    ctx, attn, _ = cross_attention(q, K, V)\n",
    "    attn_history.append(attn)\n",
    "\n",
    "    # 3) predict next token\n",
    "    logits = ctx @ W_out + b_out\n",
    "\n",
    "    # disallow BOS always\n",
    "    logits[tok2id[\"<BOS>\"]] = -1e9\n",
    "    # disallow EOS for the first 4 steps, allow at t >= 4\n",
    "    if t < 4:\n",
    "        logits[tok2id[\"<EOS>\"]] = -1e9\n",
    "\n",
    "    probs = softmax(logits[None, :], axis=-1)[0]\n",
    "    next_id = int(np.argmax(probs))\n",
    "    generated.append(next_id)\n",
    "\n",
    "    if id2tok[next_id] == \"<EOS>\":\n",
    "        break\n",
    "\n",
    "    # 4) update state to the embedding of the token we just emitted\n",
    "    state = E_dec[id2tok[next_id]]\n",
    "\n",
    "# ----------------------------\n",
    "# Display\n",
    "# ----------------------------\n",
    "gen_tokens = [id2tok[i] for i in generated]\n",
    "print(\"Document: \", \" \".join(doc_tokens))\n",
    "print(\"Summary  : \", \" \".join(gen_tokens))\n",
    "\n",
    "print(\"\\nCross-attention weights per step (rows=steps, cols=document tokens):\")\n",
    "header = \"step  | \" + \" | \".join([f\"{w:>6}\" for w in doc_tokens])\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for t, a in enumerate(attn_history, start=1):\n",
    "    row = f\"{t:>4}  | \" + \" | \".join([f\"{w:>6.3f}\" for w in a])\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91390c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
