{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e227fc",
   "metadata": {},
   "source": [
    "# Flash attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f6e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Attention Output:\n",
      "[[0.55259814 0.41700637 0.25999533 0.4921267  0.46558592 0.51640672]\n",
      " [0.55160996 0.40042111 0.25835317 0.47554091 0.52187624 0.51087415]\n",
      " [0.56089783 0.40232874 0.27235195 0.47382213 0.48810883 0.50043211]\n",
      " [0.5564651  0.41181309 0.25998126 0.4760774  0.50894969 0.4904798 ]\n",
      " [0.54653764 0.41196281 0.25553554 0.48323978 0.49929823 0.51021444]\n",
      " [0.55621186 0.41134108 0.25624623 0.47961047 0.51108311 0.49819644]\n",
      " [0.55028725 0.40710339 0.26557556 0.47792453 0.48964641 0.50576986]\n",
      " [0.55683063 0.41829481 0.24900266 0.48628646 0.50820815 0.50049716]]\n",
      "\n",
      "FlashAttention Output:\n",
      "[[0.55259814 0.41700637 0.25999533 0.4921267  0.46558592 0.51640672]\n",
      " [0.55160996 0.40042111 0.25835317 0.47554091 0.52187624 0.51087415]\n",
      " [0.56089783 0.40232874 0.27235195 0.47382213 0.48810883 0.50043211]\n",
      " [0.5564651  0.41181309 0.25998126 0.4760774  0.50894969 0.4904798 ]\n",
      " [0.54653764 0.41196281 0.25553554 0.48323978 0.49929823 0.51021444]\n",
      " [0.55621186 0.41134108 0.25624623 0.47961047 0.51108311 0.49819644]\n",
      " [0.55028725 0.40710339 0.26557556 0.47792453 0.48964641 0.50576986]\n",
      " [0.55683063 0.41829481 0.24900266 0.48628646 0.50820815 0.50049716]]\n",
      "\n",
      "Max difference between outputs:\n",
      "1.6653345369377348e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standard attention (naive implementation) - for reference\n",
    "def attention_naive(Q, K, V):\n",
    "    # Q, K: shape (N, d), V: shape (N, d_v)\n",
    "    N, d = Q.shape\n",
    "    # Compute all pairwise scores (N x N matrix)\n",
    "    scores = Q @ K.T / np.sqrt(d)\n",
    "    # Apply softmax to each row to get attention weights\n",
    "    weights = np.exp(scores)\n",
    "    weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "    # Multiply weights by V to get outputs\n",
    "    return weights @ V\n",
    "\n",
    "# FlashAttention-style attention (tiled softmax computation)\n",
    "def attention_flash(Q, K, V, block_size):\n",
    "    N, d = Q.shape\n",
    "    _, d_v = V.shape\n",
    "    output = np.zeros((N, d_v))\n",
    "    max_scores = np.full(N, -np.inf)\n",
    "    # 1. First pass: find max score per query across all key blocks\n",
    "    for j in range(0, N, block_size):\n",
    "        scores_block = (Q @ K[j:j+block_size].T) / np.sqrt(d)\n",
    "        max_scores = np.maximum(max_scores, scores_block.max(axis=1))\n",
    "    # 2. Second pass: accumulate exp(scores) and output\n",
    "    exp_sums = np.zeros(N)\n",
    "    for j in range(0, N, block_size):\n",
    "        scores_block = (Q @ K[j:j+block_size].T) / np.sqrt(d)\n",
    "        # subtract max for numerical stability\n",
    "        scores_block -= max_scores[:, None]\n",
    "        exp_block = np.exp(scores_block)\n",
    "        exp_sums += exp_block.sum(axis=1)\n",
    "        output += exp_block @ V[j:j+block_size]\n",
    "    # 3. Normalize output by total sum of exponentials\n",
    "    output = (output.T / exp_sums).T\n",
    "    return output\n",
    "    \n",
    "# --- Example usage and output ---\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "\n",
    "    N = 8      # number of tokens\n",
    "    d = 4      # dimension of Q/K\n",
    "    d_v = 6    # dimension of V\n",
    "    block_size = 4\n",
    "\n",
    "    Q = np.random.rand(N, d)\n",
    "    K = np.random.rand(N, d)\n",
    "    V = np.random.rand(N, d_v)\n",
    "\n",
    "    out_naive = attention_naive(Q, K, V)\n",
    "    out_flash = attention_flash(Q, K, V, block_size)\n",
    "\n",
    "    print(\"Naive Attention Output:\")\n",
    "    print(out_naive)\n",
    "\n",
    "    print(\"\\nFlashAttention Output:\")\n",
    "    print(out_flash)\n",
    "\n",
    "    print(\"\\nMax difference between outputs:\")\n",
    "    print(np.max(np.abs(out_naive - out_flash)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174e236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
