{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc7e50e",
   "metadata": {},
   "source": [
    "# Single self-attention implementation\n",
    "\n",
    "- A batch of input sequences, each consisting of embeddings for tokens.\n",
    "- Each embedding has a certain dimensionality (embed_dim).\n",
    "- The input tensor dimensions are (batch_size, sequence_length, embed_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(SingleHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Linear layers to create queries, keys, and values\n",
    "        self.query_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final linear layer after attention\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # Compute queries, keys, values\n",
    "        queries = self.query_linear(x)  # (batch_size, seq_length, embed_dim)\n",
    "        keys = self.key_linear(x)       # (batch_size, seq_length, embed_dim)\n",
    "        values = self.value_linear(x)   # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (embed_dim ** 0.5)\n",
    "\n",
    "        # Apply mask if provided (useful for ignoring padding tokens)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "\n",
    "        # Normalize scores into probabilities\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Final linear layer\n",
    "        output = self.fc_out(attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d441dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==== Single-Head Self-Attention ====\n",
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(SingleHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, words=None, mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "        print(f\"\\n--- INPUT EMBEDDINGS ---\")\n",
    "        for b in range(batch_size):\n",
    "            print(f\"Sentence {b+1}:\")\n",
    "            for i, word in enumerate(words[b]):\n",
    "                print(f\"  {word:10s} -> {x[b, i].detach().numpy()}\")\n",
    "\n",
    "        # Q, K, V\n",
    "        queries = self.query_linear(x)\n",
    "        keys = self.key_linear(x)\n",
    "        values = self.value_linear(x)\n",
    "\n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (embed_dim ** 0.5)\n",
    "\n",
    "        # Apply mask if any\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "\n",
    "        # Softmax → attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Pretty-print attention map\n",
    "        print(f\"\\n--- ATTENTION MAP ---\")\n",
    "        for b in range(batch_size):\n",
    "            print(f\"Sentence {b+1}:\")\n",
    "            for i, word in enumerate(words[b]):\n",
    "                attn_dist = attention_weights[b, i].detach().numpy()\n",
    "                looked_at = [f\"{words[b][j]} ({attn_dist[j]:.2f})\" for j in range(seq_length)]\n",
    "                print(f'  \"{word}\" looks at: ' + \", \".join(looked_at))\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Final projection\n",
    "        output = self.fc_out(attention_output)\n",
    "\n",
    "        print(f\"\\n--- FINAL OUTPUT ---\")\n",
    "        for b in range(batch_size):\n",
    "            print(f\"Sentence {b+1}:\")\n",
    "            for i, word in enumerate(words[b]):\n",
    "                print(f\"  {word:10s} -> {output[b, i].detach().numpy()}\")\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29fea599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INPUT EMBEDDINGS ---\n",
      "Sentence 1:\n",
      "  i          -> [-0.09334823  0.6870502  -0.83831537  0.00089182]\n",
      "  love       -> [ 0.8418941  -0.40003455  1.039462    0.3581531 ]\n",
      "  pizza      -> [0.07324605 1.1133184  0.28226727 0.43422565]\n",
      "Sentence 2:\n",
      "  i          -> [-0.09334823  0.6870502  -0.83831537  0.00089182]\n",
      "  hate       -> [ 0.20641631 -0.33344787 -0.42883     0.23291828]\n",
      "  broccoli   -> [ 0.79688716 -0.18484132 -0.3701471  -1.2102813 ]\n",
      "\n",
      "--- ATTENTION MAP ---\n",
      "Sentence 1:\n",
      "  \"i\" looks at: i (0.29), love (0.33), pizza (0.38)\n",
      "  \"love\" looks at: i (0.25), love (0.43), pizza (0.32)\n",
      "  \"pizza\" looks at: i (0.24), love (0.39), pizza (0.36)\n",
      "Sentence 2:\n",
      "  \"i\" looks at: i (0.36), hate (0.32), broccoli (0.32)\n",
      "  \"hate\" looks at: i (0.32), hate (0.31), broccoli (0.37)\n",
      "  \"broccoli\" looks at: i (0.38), hate (0.35), broccoli (0.27)\n",
      "\n",
      "--- FINAL OUTPUT ---\n",
      "Sentence 1:\n",
      "  i          -> [ 0.00859527 -0.4691878  -0.10628858 -0.708125  ]\n",
      "  love       -> [ 0.02552723 -0.4547485  -0.09126757 -0.6452503 ]\n",
      "  pizza      -> [ 0.01757566 -0.46060064 -0.10005993 -0.65962607]\n",
      "Sentence 2:\n",
      "  i          -> [ 0.16048461 -0.38403288  0.0678077  -1.0530016 ]\n",
      "  hate       -> [ 0.18075228 -0.3685905   0.07856187 -1.0545124 ]\n",
      "  broccoli   -> [ 0.13976508 -0.39992458  0.05830789 -1.0472741 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0086, -0.4692, -0.1063, -0.7081],\n",
       "         [ 0.0255, -0.4547, -0.0913, -0.6453],\n",
       "         [ 0.0176, -0.4606, -0.1001, -0.6596]],\n",
       "\n",
       "        [[ 0.1605, -0.3840,  0.0678, -1.0530],\n",
       "         [ 0.1808, -0.3686,  0.0786, -1.0545],\n",
       "         [ 0.1398, -0.3999,  0.0583, -1.0473]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== Vocabulary ====\n",
    "vocab = {\n",
    "    \"i\": 0,\n",
    "    \"love\": 1,\n",
    "    \"pizza\": 2,\n",
    "    \"you\": 3,\n",
    "    \"hate\": 4,\n",
    "    \"broccoli\": 5\n",
    "}\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 4\n",
    "\n",
    "# ==== Embedding layer ====\n",
    "embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# ==== Sentences ====\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"pizza\"],\n",
    "    [\"i\", \"hate\", \"broccoli\"]\n",
    "]\n",
    "token_ids = torch.tensor([[vocab[w] for w in sent] for sent in sentences])\n",
    "mask = torch.ones(token_ids.shape, dtype=torch.int)\n",
    "\n",
    "# ==== Convert to embeddings ====\n",
    "embedded_sentences = embedding_layer(token_ids)\n",
    "\n",
    "# ==== Run attention ====\n",
    "torch.manual_seed(0)\n",
    "attn = SingleHeadSelfAttention(embed_dim=embed_dim)\n",
    "attn(embedded_sentences, words=sentences, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf397695",
   "metadata": {},
   "source": [
    "# Multi-head attention\n",
    "\n",
    "Suppose you’re given:\n",
    "- A batch of sequences of token embeddings\n",
    "- The number of attention heads (num_heads)\n",
    "- The dimension of embeddings (embed_dim), which must be divisible by num_heads\n",
    "- Input tensor dimensions: (batch_size, sequence_length, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f800517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimension of input embeddings.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Validate embedding dimension divisibility\n",
    "        if self.head_dim * num_heads != embed_dim:\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads.\")\n",
    "\n",
    "        # TODO: Define linear layers for queries, keys, values, and output projection\n",
    "\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Final linear layer to combine heads\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Perform multi-head self-attention.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_length, embed_dim).\n",
    "            mask: Optional tensor to mask specific tokens.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_length, embed_dim), attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Implement multi-head self-attention logic here\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # Project inputs to queries, keys, and values, and split embeddings into multiple heads\n",
    "        queries = self.query_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for multi-head attention computation (batch_size, num_heads, seq_length, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention scores per head\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(2)  # shape: (batch_size, 1, 1, seq_length)\n",
    "            attention_scores = attention_scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalize scores into attention probabilities\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Concatenate heads' outputs back to original embedding dimension\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        concatenated_output = attention_output.reshape(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Final linear layer to refine concatenated outputs\n",
    "        output = self.fc_out(concatenated_output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Validate embedding dimension divisibility\n",
    "        if self.head_dim * num_heads != embed_dim:\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads.\")\n",
    "\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        # Final linear layer to combine heads\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # Project inputs to queries, keys, and values, and split embeddings into multiple heads\n",
    "        queries = self.query_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        keys = self.key_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        values = self.value_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for multi-head attention computation (batch_size, num_heads, seq_length, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention scores per head\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(2)  # shape: (batch_size, 1, 1, seq_length)\n",
    "            attention_scores = attention_scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "\n",
    "        # Normalize scores into attention probabilities\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Concatenate heads' outputs back to original embedding dimension\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        concatenated_output = attention_output.reshape(batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Final linear layer to refine concatenated outputs\n",
    "        output = self.fc_out(concatenated_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5489ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==== Your Multi-Head Self-Attention ====\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if self.head_dim * num_heads != embed_dim:\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads.\")\n",
    "\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        Q = self.query_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value_proj(x).reshape(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: (batch, seq) -> (batch, 1, 1, seq) to broadcast over heads and query positions\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)   # (batch, heads, seq, seq)\n",
    "        out = torch.matmul(weights, V)        # (batch, heads, seq, head_dim)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().reshape(batch_size, seq_length, embed_dim)\n",
    "        out = self.fc_out(out)\n",
    "        return out, weights  # return weights so we can pretty-print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d14dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INPUT WORDS & EMBEDDINGS ===\n",
      "Sentence 1:\n",
      "  i          -> [-1.1258398  -1.1523602  -0.25057858 -0.43387884  0.84871036  0.6920092\n",
      " -0.31601277 -2.1152196 ]\n",
      "  love       -> [ 0.32227492 -1.2633348   0.3499832   0.3081339   0.11984151  1.2376579\n",
      "  1.1167772  -0.24727765]\n",
      "  pizza      -> [-1.3526537  -1.6959313   0.5666505   0.7935084   0.59883946 -1.5550951\n",
      " -0.3413603   1.8530061 ]\n",
      "Sentence 2:\n",
      "  i          -> [-1.1258398  -1.1523602  -0.25057858 -0.43387884  0.84871036  0.6920092\n",
      " -0.31601277 -2.1152196 ]\n",
      "  hate       -> [-0.6135831   0.03159274 -0.49267703  0.24841475  0.43969586  0.11241119\n",
      "  0.64079237  0.44115627]\n",
      "  broccoli   -> [-0.10230965  0.792444   -0.28966758  0.05250749  0.5228604   2.3022053\n",
      " -1.4688939  -1.5866888 ]\n",
      "\n",
      "=== PER-HEAD ATTENTION MAPS (probabilities) ===\n",
      "\n",
      "Sentence 1: i love pizza\n",
      "  Head 0:\n",
      "    \"i\" looks at: i (0.33), love (0.31), pizza (0.36)\n",
      "    \"love\" looks at: i (0.33), love (0.30), pizza (0.37)\n",
      "    \"pizza\" looks at: i (0.36), love (0.30), pizza (0.34)\n",
      "  Head 1:\n",
      "    \"i\" looks at: i (0.44), love (0.18), pizza (0.38)\n",
      "    \"love\" looks at: i (0.30), love (0.45), pizza (0.26)\n",
      "    \"pizza\" looks at: i (0.28), love (0.38), pizza (0.34)\n",
      "\n",
      "Sentence 2: i hate broccoli\n",
      "  Head 0:\n",
      "    \"i\" looks at: i (0.35), hate (0.36), broccoli (0.30)\n",
      "    \"hate\" looks at: i (0.32), hate (0.32), broccoli (0.36)\n",
      "    \"broccoli\" looks at: i (0.40), hate (0.38), broccoli (0.22)\n",
      "  Head 1:\n",
      "    \"i\" looks at: i (0.44), hate (0.26), broccoli (0.31)\n",
      "    \"hate\" looks at: i (0.31), hate (0.34), broccoli (0.35)\n",
      "    \"broccoli\" looks at: i (0.45), hate (0.26), broccoli (0.30)\n",
      "\n",
      "=== FINAL OUTPUT VECTORS (after concat + fc_out) ===\n",
      "Sentence 1:\n",
      "  i          -> [-0.13795751 -0.26501223  0.03185483  0.27141413  0.0139516   0.03061761\n",
      " -0.2897094   0.2586206 ]\n",
      "  love       -> [-0.15753919 -0.22083187 -0.02306039  0.33262262 -0.01342669  0.05675994\n",
      " -0.24400553  0.2776681 ]\n",
      "  pizza      -> [-0.14169443 -0.24183795 -0.00061824  0.30149636  0.00711769  0.03541858\n",
      " -0.2673983   0.26485148]\n",
      "Sentence 2:\n",
      "  i          -> [-0.256302   -0.30446398  0.03422173  0.2165659  -0.03190425  0.04715188\n",
      " -0.263952    0.13009045]\n",
      "  hate       -> [-0.23083222 -0.31619498  0.04767641  0.18969168 -0.00763118  0.02790348\n",
      " -0.2726216   0.09489416]\n",
      "  broccoli   -> [-0.25133517 -0.3015918   0.02702024  0.2238084  -0.03096928  0.04057232\n",
      " -0.27193365  0.15786737]\n",
      "\n",
      "=== TOP-1 TARGET PER TOKEN PER HEAD ===\n",
      "Sentence 1:\n",
      "  Head 0: i→pizza, love→pizza, pizza→i\n",
      "  Head 1: i→i, love→love, pizza→love\n",
      "Sentence 2:\n",
      "  Head 0: i→hate, hate→broccoli, broccoli→i\n",
      "  Head 1: i→i, hate→broccoli, broccoli→i\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== A tiny driver that prints per-head attention nicely ====\n",
    "def run_demo_pretty_print_mha():\n",
    "    # Tiny vocabulary\n",
    "    vocab = {\"i\":0, \"love\":1, \"pizza\":2, \"you\":3, \"hate\":4, \"broccoli\":5}\n",
    "    sentences = [\n",
    "        [\"i\", \"love\", \"pizza\"],\n",
    "        [\"i\", \"hate\", \"broccoli\"]\n",
    "    ]\n",
    "    token_ids = torch.tensor([[vocab[w] for w in s] for s in sentences])  # (batch=2, seq=3)\n",
    "    mask = torch.ones_like(token_ids, dtype=torch.int)  # keep it simple: no padding masked out\n",
    "\n",
    "    # Small, readable dims\n",
    "    embed_dim = 8\n",
    "    num_heads = 2  # head_dim = 4\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Embedding + attention\n",
    "    embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "    x = embedding(token_ids)  # (batch, seq, embed_dim)\n",
    "\n",
    "    print(\"\\n=== INPUT WORDS & EMBEDDINGS ===\")\n",
    "    for b, sent in enumerate(sentences):\n",
    "        print(f\"Sentence {b+1}:\")\n",
    "        for i, w in enumerate(sent):\n",
    "            print(f\"  {w:10s} -> {x[b, i].detach().numpy()}\")\n",
    "\n",
    "    mha = MultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "    out, attn_weights = mha(x, mask=mask)  # out: (batch, seq, embed_dim); attn_weights: (batch, heads, seq, seq)\n",
    "\n",
    "    # Pretty print attention per head\n",
    "    print(\"\\n=== PER-HEAD ATTENTION MAPS (probabilities) ===\")\n",
    "    B, H, S, _ = attn_weights.shape\n",
    "    for b in range(B):\n",
    "        print(f\"\\nSentence {b+1}: {' '.join(sentences[b])}\")\n",
    "        for h in range(H):\n",
    "            print(f\"  Head {h}:\")\n",
    "            for i in range(S):\n",
    "                dist = attn_weights[b, h, i].detach().numpy()\n",
    "                looked_at = [f\"{sentences[b][j]} ({dist[j]:.2f})\" for j in range(S)]\n",
    "                print(f'    \"{sentences[b][i]}\" looks at: ' + \", \".join(looked_at))\n",
    "\n",
    "    # Final outputs\n",
    "    print(\"\\n=== FINAL OUTPUT VECTORS (after concat + fc_out) ===\")\n",
    "    for b, sent in enumerate(sentences):\n",
    "        print(f\"Sentence {b+1}:\")\n",
    "        for i, w in enumerate(sent):\n",
    "            print(f\"  {w:10s} -> {out[b, i].detach().numpy()}\")\n",
    "\n",
    "    # Optional: top-1 attention target per token per head\n",
    "    print(\"\\n=== TOP-1 TARGET PER TOKEN PER HEAD ===\")\n",
    "    top_idx = attn_weights.argmax(dim=-1)  # (batch, heads, seq)\n",
    "    for b in range(B):\n",
    "        print(f\"Sentence {b+1}:\")\n",
    "        for h in range(H):\n",
    "            picks = [sentences[b][top_idx[b, h, i].item()] for i in range(S)]\n",
    "            srcs  = sentences[b]\n",
    "            pairs = [f'{srcs[i]}→{picks[i]}' for i in range(S)]\n",
    "            print(f\"  Head {h}: \" + \", \".join(pairs))\n",
    "\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo_pretty_print_mha()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f6e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
